# -*- coding: utf-8 -*-
"""python ai_eyes_for_the_blind_google_gemma_3n_impact_challenge.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DrmJqmlnvTyKYffBps1bjPLs7lUb-LFU
"""

# -*- coding: utf-8 -*-
"""python ai_eyes_for_the_blind_google_gemma_3n_impact_challenge.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DrmJqmlnvTyKYffBps1bjPLs7lUb-LFU
"""

"""
The MIT License

Copyright 2025 Mohammad Parham Dehghan

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ‚ÄúSoftware‚Äù), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
"""

# -*- coding: utf-8 -*-
"""VisionAid+DeafAid App Optimized for Gemma Hackathon with Video and Object Detection for Colab"""

# Suppress warnings
import warnings
warnings.filterwarnings('ignore')

# Install prerequisites for opencv and geocoder
!apt update -q && apt install -y libopencv-dev python3-opencv -q

# Install tesseract OCR
!apt install tesseract-ocr -q

# Update torch to compatible version
!pip install -q torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1

# Reformed pip install to avoid dependency conflicts in Colab
!pip install -q numpy==1.26.4 ultralytics==8.2.58 flask==2.2.5 transformers==4.42.4 pillow==10.4.0 gTTS==2.3.2 speechrecognition==3.10.0 pycocotools==2.0.7 pyngrok==6.0.0 opencv-python==4.10.0.84 geocoder==1.38.1 flask-cors==4.0.1 pytesseract -q

# Install Doctr for OCR
!pip install -q python-doctr

from flask_cors import CORS
import os
import time
import math
import numpy as np
from flask import Flask, request, render_template_string, Response, jsonify, send_file
from transformers import AutoTokenizer, AutoModelForCausalLM, BlipProcessor, BlipForConditionalGeneration, pipeline
from PIL import Image
from io import BytesIO
from base64 import b64decode, b64encode
from gtts import gTTS
import torch
from torch.quantization import quantize_dynamic
import speech_recognition as sr
import cv2
import geocoder
import re
import threading
from collections import Counter
from ultralytics import YOLO
from pyngrok import ngrok
import glob
import pytesseract
from doctr.io import DocumentFile
from doctr.models import ocr_predictor
import io

# Resolve ngrok config warning by moving legacy file
os.system('mkdir -p /root/.config/ngrok')
os.system('mv /root/.ngrok2/ngrok.yml /root/.config/ngrok/ngrok.yml || true')

# Set HF token securely
os.environ["HF_TOKEN"] = "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"

# Flask app
app = Flask(__name__)
CORS(app)

# Device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Download YOLO model if not exist
if not os.path.exists('yolov8n.pt'):
    os.system('wget -q https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt')
    if not os.path.exists('yolov8n.pt'):
        print("Failed to download yolov8n.pt. Please download manually.")
else:
    print("yolov8n.pt already exists.")

def load_models():
    try:
        start_time = time.time()
        print("Loading BLIP processor and model...")
        blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
        blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device)
        print("Loading Gemma model and tokenizer...")
        gemma_model = AutoModelForCausalLM.from_pretrained("google/gemma-2b").to(device)
        gemma_tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b")
        print("Loading Whisper model...")
        whisper = pipeline("automatic-speech-recognition", model="openai/whisper-tiny")
        print("Loading YOLO model...")
        yolo_model = YOLO("yolov8n.pt").to(device)
        print("Loading MiDaS depth model...")
        midas = torch.hub.load("intel-isl/MiDaS", "MiDaS_small").to(device)
        midas_transforms = torch.hub.load("intel-isl/MiDaS", "transforms")
        midas_transform = midas_transforms.small_transform
        print(f"Model loading time: {time.time() - start_time:.2f} seconds")
        return blip_processor, blip_model, gemma_tokenizer, gemma_model, whisper, yolo_model, midas, midas_transform
    except Exception as e:
        print(f"Error loading models: {str(e)}")
        return None, None, None, None, None, None, None, None

# Call models
blip_processor, blip_model, gemma_tokenizer, gemma_model, whisper, yolo_model, midas, midas_transform = load_models()

def transcribe_audio(audio_path, language='en'):
    try:
        if whisper is None:
            return "Transcription model not loaded."
        result = whisper(audio_path, language='fa' if language == 'fa' else 'en')
        text = result['text'].strip().lower()
        if gemma_model and gemma_tokenizer:
            with torch.no_grad():
                prompt = f"Refine this transcribed speech for clarity in {language}: {text}"
                inputs = gemma_tokenizer(prompt, return_tensors="pt").to(device)
                output = gemma_model.generate(**inputs, max_new_tokens=50, temperature=0.7, do_sample=True)
                text = gemma_tokenizer.decode(output[0][len(inputs.input_ids[0]):], skip_special_tokens=True)
        return text
    except Exception as e:
        print(f"Audio transcription error: {str(e)}")
        return "Transcription failed. Please use text input."

def clean_old_files(directory='saved_files', max_age_seconds=600):
    try:
        current_time = time.time()
        for file_path in glob.glob(os.path.join(directory, '*')) + glob.glob('audio/*.mp3'):
            if os.path.getmtime(file_path) < current_time - max_age_seconds:
                os.remove(file_path)
                print(f"Deleted old file: {file_path}")
    except Exception as e:
        print(f"Error cleaning old files: {str(e)}")

def generate_system_audio(message, language='en'):
    audio_path = f"audio/system_{int(time.time())}.mp3"
    os.makedirs('audio', exist_ok=True)
    try:
        tts = gTTS(text=message, lang=language, slow=False, tld='co.uk')
        tts.save(audio_path)
        return audio_path
    except Exception as e:
        print(f"gTTS error: {str(e)}")
        return None

def estimate_distance(depth_map, bbox, focal_length=600, avg_step_length=0.75):
    try:
        x1, y1, x2, y2 = map(int, bbox)
        region_depth = depth_map[y1:y2, x1:x2]
        if region_depth.size == 0:
            return "Unknown"
        avg_depth = np.mean(region_depth)
        distance_m = focal_length / avg_depth if avg_depth > 0 else float('inf')
        steps = int(distance_m / avg_step_length) if distance_m != float('inf') else "Far away"
        return f"{steps} steps away"
    except Exception as e:
        return f"Distance error: {str(e)}"

def extract_text_from_image(image):
    try:
        # ÿ∞ÿÆ€åÿ±Ÿá ÿ™ÿµŸà€åÿ± ŸÖŸàŸÇÿ™
        temp_path = 'temp_image.png'
        image.save(temp_path)

        # ÿ®ÿßÿ±⁄Øÿ∞ÿßÿ±€å ÿ™ÿµŸà€åÿ± ÿ®ÿß Doctr
        doc = DocumentFile.from_images(temp_path)

        # ŸÖÿØŸÑ OCR (ÿ®ÿ±ÿß€å ÿßŸÜ⁄ØŸÑ€åÿ≥€å)
        model = ocr_predictor(det_arch='db_resnet50', reco_arch='crnn_vgg16_bn', pretrained=True)

        # ÿ¥ŸÜÿßÿ≥ÿß€å€å ŸÖÿ™ŸÜ
        result = model(doc)
        text = result.render().strip()

        # Ÿæÿß⁄© ⁄©ÿ±ÿØŸÜ ŸÅÿß€åŸÑ ŸÖŸàŸÇÿ™
        os.remove(temp_path)

        return text or "No text detected."
    except Exception as e:
        return f"OCR error: {str(e)}"

def generate_subtitles(video_path, language='en'):
    try:
        cap = cv2.VideoCapture(video_path)
        audio_path = f"temp_audio_{int(time.time())}.wav"
        os.system(f"ffmpeg -i {video_path} -vn -acodec pcm_s16le -ar 16000 -ac 1 {audio_path} -y -loglevel quiet")
        subtitles = transcribe_audio(audio_path, language)
        if os.path.exists(audio_path):
            os.remove(audio_path)
        return subtitles
    except Exception as e:
        print(f"Subtitles generation error: {str(e)}")
        return "Subtitles generation failed."

def process_image_with_objects(image, language='en', user_type='blind', query=''):
    try:
        is_read_text = (('md read text' in query.lower() or 'md ÿÆŸàÿßŸÜÿØŸÜ ŸÖÿ™ŸÜ' in query.lower()) or ('read text' in query.lower() or 'ÿÆŸàÿßŸÜÿØŸÜ ŸÖÿ™ŸÜ' in query.lower()))

        if is_read_text:
            # ŸÅŸÇÿ∑ ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ŸÖÿ™ŸÜÿå ÿ®ÿØŸàŸÜ ÿ™Ÿàÿµ€åŸÅ ÿ™ÿµŸà€åÿ±
            text = extract_text_from_image(image)
            description = f"Extracted text: {text} " if language == 'en' else f"ŸÖÿ™ŸÜ ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨‚Äåÿ¥ÿØŸá: {text} "
            generate_system_audio(text, language)
            return description, None, None, []

        # Ÿæÿ±ÿØÿßÿ≤ÿ¥ ÿπÿßÿØ€å ÿ®ÿ±ÿß€å ŸÖŸàÿßÿ±ÿØ ÿØ€å⁄Øÿ±
        if yolo_model is None:
            return "Object detection model not loaded.", None, None, []
        results = yolo_model.predict(image, imgsz=640, conf=0.6)
        boxes = results[0].boxes.xyxy.cpu().numpy()
        confidences = results[0].boxes.conf.cpu().numpy()
        class_names = [results[0].names[int(cls)] for cls in results[0].boxes.cls]
        detected_objects = [name for name, conf in zip(class_names, confidences) if conf > 0.6]
        object_counts = Counter(detected_objects)
        img_array = np.array(image)
        img_array = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
        objects = []
        for box, cls, conf in zip(boxes, results[0].boxes.cls, confidences):
            if conf > 0.6:
                x1, y1, x2, y2 = map(int, box)
                class_name = results[0].names[int(cls)]
                cv2.rectangle(img_array, (x1, y1), (x2, y2), (0, 0, 255), 2)
                cv2.putText(img_array, f"{class_name} ({conf:.2f})", (x1, y1 - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
                objects.append({
                    'name': class_name,
                    'x1': float(x1),
                    'y1': float(y1),
                    'x2': float(x2),
                    'y2': float(y2),
                    'new': True
                })
        if midas and midas_transform:
            img = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)
            input_batch = midas_transform(img).to(device)
            with torch.no_grad():
                depth_map = midas(input_batch).cpu().numpy().squeeze()
            depth_map = cv2.resize(depth_map, (img.shape[1], img.shape[0]))
        else:
            depth_map = None
        processed_image = Image.fromarray(cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB))
        os.makedirs('saved_files', exist_ok=True)
        saved_file_path = f"saved_files/processed_{int(time.time())}.jpg"
        processed_image.save(saved_file_path, quality=90)
        base64_file = b64encode(open(saved_file_path, 'rb').read()).decode('utf-8')
        description = ""
        if detected_objects:
            description += f"Detected {len(detected_objects)} objects: " if language == 'en' else f"{len(detected_objects)} ÿ¥€åÿ° ÿ¥ŸÜÿßÿ≥ÿß€å€å ÿ¥ÿØ: "
            description += ', '.join([f"{count} {obj}" if count > 1 else obj for obj, count in object_counts.items()]) + ". "
        for i, (box, cls, conf) in enumerate(zip(boxes, results[0].boxes.cls, confidences)):
            if conf > 0.6:
                dist = estimate_distance(depth_map, box, focal_length=600) if depth_map is not None else "Distance not estimated"
                objects[i]['distance'] = dist
                class_name = results[0].names[int(cls)]
                description += f"{class_name} is {dist}. " if language == 'en' else f"{class_name} ÿØÿ± {dist} ŸÇÿ±ÿßÿ± ÿØÿßÿ±ÿØ. "
        if blip_processor and blip_model:
            inputs = blip_processor(images=image, return_tensors="pt").to(device)
            caption = blip_model.generate(**inputs, max_length=100, num_beams=5, no_repeat_ngram_size=2)
            caption_text = blip_processor.decode(caption[0], skip_special_tokens=True)
            if len(caption_text.split()) < 3:
                caption_text = "A lively scene with various objects." if language == 'en' else "ÿµÿ≠ŸÜŸá‚Äåÿß€å Ÿæÿ±ÿ¨ŸÜÿ®‚ÄåŸàÿ¨Ÿàÿ¥ ÿ®ÿß ÿßÿ¥€åÿß€å ŸÖÿÆÿ™ŸÑŸÅ."
        else:
            caption_text = "Fallback: Scene with objects."
        if gemma_model and gemma_tokenizer:
            prompt = f"Refine this caption for {user_type} users in {language} to enhance accessibility: Describe key elements, spatial layout, estimated distances, and object interactions clearly and concisely, without repetition or unnecessary details: {caption_text}"
            inputs = gemma_tokenizer(prompt, return_tensors="pt").to(device)
            output = gemma_model.generate(**inputs, max_new_tokens=100, temperature=0.6, do_sample=True, no_repeat_ngram_size=3)
            caption_text = gemma_tokenizer.decode(output[0][len(inputs.input_ids[0]):], skip_special_tokens=True)
            description += f"Image description: {caption_text}. " if language == 'en' else f"ÿ™Ÿàÿµ€åŸÅ ÿ™ÿµŸà€åÿ±: {caption_text}. "
        try:
            pixels = img_array.reshape(-1, 3)
            color_counts = Counter(map(tuple, pixels))
            dominant_colors = [f"rgb{t}" for t, count in color_counts.most_common(3)]
            description += f"Dominant colors: {', '.join(dominant_colors)}. " if language == 'en' else f"ÿ±ŸÜ⁄Ø‚ÄåŸáÿß€å ÿ∫ÿßŸÑÿ®: {', '.join(dominant_colors)}. "
        except Exception as e:
            description += f"Color detection error: {str(e)}. " if language == 'en' else f"ÿÆÿ∑ÿß€å ÿ™ÿ¥ÿÆ€åÿµ ÿ±ŸÜ⁄Ø: {str(e)}. "
        return description, saved_file_path, base64_file, objects
    except Exception as e:
        print(f"Image processing error: {str(e)}")
        return f"Image processing error: {str(e)}", None, None, []

def process_video(video_path, language='en', user_type='blind'):
    try:
        if yolo_model is None:
            return "Object detection model not loaded.", None, None, []
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            return "Failed to open video.", None, None, []
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        os.makedirs('saved_files', exist_ok=True)
        saved_video_path = f"saved_files/processed_video_{int(time.time())}.mp4"
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(saved_video_path, fourcc, fps, (width, height))
        description = ""
        frame_descriptions = []
        all_objects = []
        previous_objects = set()
        previous_boxes = []
        step = 5
        for i in range(0, frame_count, step):
            cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            ret, frame = cap.read()
            if not ret:
                break
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            image = Image.fromarray(frame_rgb).resize((640, 480))
            results = yolo_model.predict(image, imgsz=640, conf=0.6)
            boxes = results[0].boxes.xyxy.cpu().numpy()
            confidences = results[0].boxes.conf.cpu().numpy()
            class_names = [results[0].names[int(cls)] for cls in results[0].boxes.cls]
            detected_objects = set(name for name, conf in zip(class_names, confidences) if conf > 0.6)
            object_counts = Counter(detected_objects)
            if midas and midas_transform:
                img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                input_batch = midas_transform(img).to(device)
                with torch.no_grad():
                    depth_map = midas(input_batch).cpu().numpy().squeeze()
                depth_map = cv2.resize(depth_map, (width, height))
            else:
                depth_map = None
            if detected_objects != previous_objects:
                scale_x = width / 640
                scale_y = height / 480
                objects = []
                current_boxes = []
                for box, cls, conf in zip(boxes, results[0].boxes.cls, confidences):
                    if conf > 0.6:
                        x1, y1, x2, y2 = map(int, box)
                        x1, x2 = int(x1 * scale_x), int(x2 * scale_x)
                        y1, y2 = int(y1 * scale_y), int(y2 * scale_y)
                        class_name = results[0].names[int(cls)]
                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)
                        cv2.putText(frame, f"{class_name} ({conf:.2f})", (x1, y1 - 10),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
                        dist = estimate_distance(depth_map, [x1, y1, x2, y2], focal_length=600) if depth_map is not None else "Distance not estimated"
                        objects.append({
                            'name': class_name,
                            'x1': float(x1),
                            'y1': float(y1),
                            'x2': float(x2),
                            'y2': float(y2),
                            'new': True,
                            'distance': dist
                        })
                        current_boxes.append({'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2, 'class_name': class_name, 'conf': conf})
                all_objects.extend(objects)
                previous_boxes = current_boxes
                frame_description = f"Frame {i+1}: "
                if detected_objects:
                    frame_description += f"Detected {len(detected_objects)} unique objects: " if language == 'en' else f"{len(detected_objects)} ÿ¥€åÿ° ŸÖŸÜÿ≠ÿµÿ± ÿ®Ÿá ŸÅÿ±ÿØ ÿ¥ŸÜÿßÿ≥ÿß€å€å ÿ¥ÿØ: "
                    frame_description += ', '.join([f"{count} {obj}" if count > 1 else obj for obj, count in object_counts.items()]) + ". "
                for obj in objects:
                    frame_description += f"{obj['name']} is {obj['distance']}. " if language == 'en' else f"{obj['name']} ÿØÿ± {obj['distance']} ŸÇÿ±ÿßÿ± ÿØÿßÿ±ÿØ. "
                if blip_processor and blip_model:
                    inputs = blip_processor(images=image, return_tensors="pt").to(device)
                    caption = blip_model.generate(**inputs, max_length=100, num_beams=5, no_repeat_ngram_size=3)
                    caption_text = blip_processor.decode(caption[0], skip_special_tokens=True)
                    if len(caption_text.split()) < 3:
                        caption_text = "A lively scene with various objects." if language == 'en' else "ÿµÿ≠ŸÜŸá‚Äåÿß€å Ÿæÿ±ÿ¨ŸÜÿ®‚ÄåŸàÿ¨Ÿàÿ¥ ÿ®ÿß ÿßÿ¥€åÿß€å ŸÖÿÆÿ™ŸÑŸÅ."
                else:
                    caption_text = "Fallback: Scene with objects."
                if gemma_model and gemma_tokenizer:
                    prompt = f"Refine this caption for {user_type} users in {language} to enhance accessibility: Describe key elements, spatial layout, estimated distances, and object interactions clearly and concisely, without repetition or unnecessary details: {caption_text}"
                    inputs = gemma_tokenizer(prompt, return_tensors="pt").to(device)
                    output = gemma_model.generate(**inputs, max_new_tokens=100, temperature=0.6, do_sample=True, no_repeat_ngram_size=3)
                    caption_text = gemma_tokenizer.decode(output[0][len(inputs.input_ids[0]):], skip_special_tokens=True)
                    frame_description += f"Description: {caption_text}. "
                frame_descriptions.append(frame_description)
                previous_objects = detected_objects
            else:
                for prev_box in previous_boxes:
                    x1, y1, x2, y2 = prev_box['x1'], prev_box['y1'], prev_box['x2'], prev_box['y2']
                    class_name, conf = prev_box['class_name'], prev_box['conf']
                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)
                    cv2.putText(frame, f"{class_name} ({conf:.2f})", (x1, y1 - 10),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
            out.write(frame)
        cap.release()
        out.release()
        base64_file = b64encode(open(saved_video_path, 'rb').read()).decode('utf-8')
        description = "Video processed." if language == 'en' else "Ÿà€åÿØ€åŸà Ÿæÿ±ÿØÿßÿ≤ÿ¥ ÿ¥ÿØ. "
        description += " ".join(frame_descriptions)
        if user_type == 'deaf':
            subtitles = generate_subtitles(video_path, language)
            description += f"\nSubtitles: {subtitles} üì¢"
            description += " (Sign language translation not implemented) ü§ü"
            description += " üö® Alert: Objects detected! üì∏"
        return description, saved_video_path, base64_file, all_objects
    except Exception as e:
        print(f"Video processing error: {str(e)}")
        return f"Video processing error: {str(e)}", None, None, []

html_content = """
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="MD: VisionAid+DeafAid - An AI-powered assistive tool for blind and deaf users with video and object detection">
    <meta name="theme-color" content="#1d4ed8">
    <title>MD: VisionAid+DeafAid</title>
    <link rel="manifest" href="/manifest.json">
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;700&family=Roboto:wght@400;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', 'Roboto', sans-serif;
            background-color: #f8fafc;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            padding: 1.5rem;
            transition: all 0.3s ease;
        }
        body.blind-mode {
            background-color: #121212;
            color: #e6f3ff;
            font-size: 2.5rem;
            line-height: 3rem;
        }
        body.deaf-mode {
            background: linear-gradient(135deg, #60a5fa, #a5b4fc);
            color: #1e293b;
            font-size: 1.125rem;
        }
        .container {
            background: #ffffff;
            border-radius: 1.5rem;
            padding: 2rem;
            box-shadow: 0 10px 20px rgba(0, 0, 0, 0.1);
            max-width: 48rem;
            width: 100%;
            transition: all 0.3s ease;
        }
        .output-container {
            border-radius: 1rem;
            padding: 1.5rem;
            background-color: #f1f5f9;
            transition: all 0.3s ease;
        }
        .output-container.blind-mode {
            background-color: #1f2937;
            border: 2px solid #60a5fa;
            box-shadow: 0 6px 12px rgba(255, 255, 255, 0.1);
            font-size: 2rem;
        }
        .output-container.deaf-mode {
            background-color: #e0f2fe;
            border: 3px solid #2563eb;
            box-shadow: 0 8px 16px rgba(0, 0, 0, 0.15);
        }
        button {
            transition: all 0.3s ease;
            border-radius: 0.75rem;
            padding: 0.75rem 1.5rem;
            font-weight: 600;
            letter-spacing: 0.025rem;
        }
        button.blind-mode {
            background-color: #2563eb;
            color: #ffffff;
            border: 2px solid #60a5fa;
        }
        button.blind-mode:hover {
            background-color: #1e40af;
            transform: translateY(-2px);
            box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2);
        }
        button.deaf-mode {
            background-color: #3b82f6;
            color: #ffffff;
            border: 2px solid #93c5fd;
        }
        button.deaf-mode:hover {
            background-color: #2563eb;
            transform: translateY(-2px);
            box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2);
        }
        select, input {
            transition: all 0.3s ease;
            border-radius: 0.75rem;
            padding: 0.75rem 1rem;
            border: 2px solid #d1d5db;
            background-color: #f9fafb;
            font-size: 1rem;
        }
        select:focus, input:focus {
            border-color: #2563eb;
            outline: none;
            box-shadow: 0 0 0 4px rgba(37, 99, 235, 0.2);
        }
        [aria-label]:focus::after {
            content: attr(aria-label);
            position: absolute;
            top: -2.5rem;
            background: #1e293b;
            color: #fff;
            padding: 0.5rem 1rem;
            border-radius: 0.5rem;
            font-size: 0.875rem;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
        }
        @media (max-width: 640px) {
            body.blind-mode {
                font-size: 1.5rem;
                line-height: 2rem;
            }
            .container {
                padding: 1.25rem;
            }
            .output-container {
                padding: 1rem;
            }
            button, select, input {
                padding: 0.5rem 0.75rem;
                font-size: 0.875rem;
            }
            h1 {
                font-size: 2rem;
            }
            p {
                font-size: 1rem;
            }
        }
        h1 {
            font-size: 2.5rem;
            font-weight: 800;
            color: #1e3a8a;
            text-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            animation: fadeIn 1s ease-in;
        }
        h3 {
            font-size: 1.25rem;
            font-weight: 700;
            color: #1e293b;
        }
        p {
            font-size: 1.125rem;
            color: #4b5563;
        }
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(-10px); }
            to { opacity: 1; transform: translateY(0); }
        }
        img, video {
            max-width: 100%;
            border-radius: 1rem;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            transition: transform 0.3s ease;
        }
        img:hover, video:hover {
            transform: scale(1.02);
        }
        .visual-alert {
            animation: flash 1s infinite;
        }
        @keyframes flash {
            0% { opacity: 1; }
            50% { opacity: 0; }
            100% { opacity: 1; }
        }
    </style>
</head>
<body class="deaf-mode min-h-screen flex flex-col items-center justify-content p-6">
    <h1 class="text-4xl font-extrabold mb-6 text-blue-800 animate-pulse" aria-label="MD: VisionAid+DeafAid Title">MD: VisionAid+DeafAid</h1>
    <p class="text-xl mb-8 text-gray-900 max-w-2xl text-center" aria-label="Description">An AI-powered assistive tool for blind and deaf users with video and object detection</p>
    <div class="container">
        <div class="mb-6">
            <label for="user_type" class="block text-lg font-semibold text-gray-900 mb-2" aria-label="User Type Label">User Type:</label>
            <select id="user_type" class="block w-full p-3 border-2 border-gray-300 rounded-lg bg-gray-50 focus:border-blue-600" onchange="updateUserMode()" aria-label="Select user type">
                <option value="blind">Blind</option>
                <option value="deaf">Deaf</option>
            </select>
        </div>
        <div class="mb-6">
            <label for="language" class="block text-lg font-semibold text-gray-900 mb-2" aria-label="Language Label">Language:</label>
            <select id="language" class="block w-full p-3 border-2 border-gray-300 rounded-lg bg-gray-50 focus:border-blue-600" aria-label="Select language">
                <option value="en">English</option>
                <option value="fa">ŸÅÿßÿ±ÿ≥€å</option>
            </select>
        </div>
        <div class="mb-6">
            <h3 class="text-xl font-semibold text-gray-900 mb-3" aria-label="Target Coordinates Section">Target Coordinates</h3>
            <input type="number" step="any" id="object_lat" placeholder="Target Latitude (e.g., 35.1357)" class="block w-full p-3 border-2 border-gray-300 rounded-lg bg-gray-50 focus:border-blue-600 mb-3" aria-label="Enter target latitude">
            <input type="number" step="any" id="object_lon" placeholder="Target Longitude (e.g., -121.8269)" class="block w-full p-3 border-2 border-gray-300 rounded-lg bg-gray-50 focus:border-blue-600" aria-label="Enter target longitude">
        </div>
        <div class="mb-6">
            <h3 class="text-xl font-semibold text-gray-900 mb-3" aria-label="User Coordinates Section">User Coordinates (Optional)</h3>
            <input type="number" step="any" id="user_lat" placeholder="Your Latitude (Optional)" class="block w-full p-3 border-2 border-gray-300 rounded-lg bg-gray-50 focus:border-blue-600 mb-3" aria-label="Enter your latitude">
            <input type="number" step="any" id="user_lon" placeholder="Your Longitude (Optional)" class="block w-full p-3 border-2 border-gray-300 rounded-lg bg-gray-50 focus:border-blue-600" aria-label="Enter your longitude">
        </div>
        <div class="mb-6">
            <h3 class="text-xl font-semibold text-gray-900 mb-3" aria-label="Input Section">Image or Video Input</h3>
            <input type="file" id="media_input" accept="image/*,video/*" class="block w-full p-2 border-2 border-gray-300 rounded-lg bg-gray-50" aria-label="Upload image or video">
        </div>
        <div class="mb-6">
            <h3 class="text-xl font-semibold text-gray-900 mb-3" aria-label="Audio Input Section">Audio Input</h3>
            <button id="listen-btn" onclick="startListening()" class="w-full bg-purple-600 text-white px-6 py-3 rounded-lg hover:bg-purple-700" aria-label="Start listening">Start Listening</button>
            <div id="recording-status" class="text-red-600 mt-3 text-center font-medium" aria-live="polite"></div>
        </div>
        <div class="mb-6">
            <h3 class="text-xl font-semibold text-gray-900 mb-3" aria-label="Query Section">Query</h3>
            <input type="text" id="query" placeholder="Ask MD (e.g., MD, process video or where am I)" class="block w-full p-3 border-2 border-gray-300 rounded-lg bg-gray-50 focus:border-blue-600" aria-label="Enter query">
            <button id="process-btn" onclick="processInput()" class="w-full mt-3 bg-blue-600 text-white px-6 py-3 rounded-lg hover:bg-blue-700" aria-label="Process input">Process</button>
        </div>
        <div class="mb-6">
            <h3 class="text-xl font-semibold text-gray-900 mb-3" aria-label="Output Section">Output</h3>
            <div id="output" class="output-container p-6 rounded-lg bg-gray-50" aria-live="polite"></div>
            <img id="processed_image" src="" style="display:none;" class="mt-4" aria-label="Processed image with detected objects">
            <video id="processed_video" controls style="display:none;" class="mt-4" aria-label="Processed video with detected objects"></video>
            <audio id="audio_output" controls class="mt-4 w-full" aria-label="Play audio output"></audio>
            <textarea id="history-log" style="display:none;"></textarea>
        </div>
    </div>
    <script>
        let listening = false;
        const recordingStatus = document.getElementById('recording-status');
        const output = document.getElementById('output');
        const audioOutput = document.getElementById('audio_output');
        const queryInput = document.getElementById('query');
        const processedImage = document.getElementById('processed_image');
        const processedVideo = document.getElementById('processed_video');
        console.log('JavaScript loaded successfully');
        const translations = {
            en: {
                title: "MD: VisionAid+DeafAid",
                description: "An AI-powered assistive tool for blind and deaf users with video and object detection",
                userTypeLabel: "User Type:",
                userTypeBlind: "Blind",
                userTypeDeaf: "Deaf",
                languageLabel: "Language:",
                targetCoords: "Target Coordinates",
                targetLat: "Target Latitude (e.g., 35.1357)",
                targetLon: "Target Longitude (e.g., -121.8269)",
                userCoords: "Your Coordinates (Optional)",
                userLat: "Your Latitude (Optional)",
                userLon: "Your Longitude (Optional)",
                mediaInput: "Image or Video Input",
                audioInput: "Audio Input",
                startListening: "Start Listening",
                querySection: "Query",
                queryPlaceholder: "Ask MD (e.g., MD, process video or where am I)",
                process: "Process",
                outputSection: "Output"
            },
            fa: {
                title: "MD: VisionAid+DeafAid",
                description: "€å⁄© ÿßÿ®ÿ≤ÿßÿ± ⁄©ŸÖ⁄©€å ŸÖÿ®ÿ™ŸÜ€å ÿ®ÿ± ŸáŸàÿ¥ ŸÖÿµŸÜŸàÿπ€å ÿ®ÿ±ÿß€å ⁄©ÿßÿ±ÿ®ÿ±ÿßŸÜ ŸÜÿßÿ®€åŸÜÿß Ÿà ŸÜÿßÿ¥ŸÜŸàÿß ÿ®ÿß ÿ™ÿ¥ÿÆ€åÿµ Ÿà€åÿØ€åŸà Ÿà ÿ¥€åÿ°",
                userTypeLabel: "ŸÜŸàÿπ ⁄©ÿßÿ±ÿ®ÿ±:",
                userTypeBlind: "ŸÜÿßÿ®€åŸÜÿß",
                userTypeDeaf: "ŸÜÿßÿ¥ŸÜŸàÿß",
                languageLabel: "ÿ≤ÿ®ÿßŸÜ:",
                targetCoords: "ŸÖÿÆÿ™ÿµÿßÿ™ ŸáÿØŸÅ",
                targetLat: "ÿπÿ±ÿ∂ ÿ¨ÿ∫ÿ±ÿßŸÅ€åÿß€å€å ŸáÿØŸÅ (ŸÖÿ´ÿßŸÑ: 35.1357)",
                targetLon: "ÿ∑ŸàŸÑ ÿ¨ÿ∫ÿ±ÿßŸÅ€åÿß€å€å ŸáÿØŸÅ (ŸÖÿ´ÿßŸÑ: -121.8269)",
                userCoords: "ŸÖÿÆÿ™ÿµÿßÿ™ ÿ¥ŸÖÿß (ÿßÿÆÿ™€åÿßÿ±€å)",
                userLat: "ÿπÿ±ÿ∂ ÿ¨ÿ∫ÿ±ÿßŸÅ€åÿß€å€å ÿ¥ŸÖÿß (ÿßÿÆÿ™€åÿßÿ±€å)",
                userLon: "ÿ∑ŸàŸÑ ÿ¨ÿ∫ÿ±ÿßŸÅ€åÿß€å€å ÿ¥ŸÖÿß (ÿßÿÆÿ™€åÿßÿ±€å)",
                mediaInput: "Ÿàÿ±ŸàÿØ€å ÿ™ÿµŸà€åÿ± €åÿß Ÿà€åÿØ€åŸà",
                audioInput: "Ÿàÿ±ŸàÿØ€å ÿµŸàÿ™€å",
                startListening: "ÿ¥ÿ±Ÿàÿπ ⁄ØŸàÿ¥ ÿØÿßÿØŸÜ",
                querySection: "Ÿæÿ±ÿ≥‚ÄåŸàÿ¨Ÿà",
                queryPlaceholder: "ÿßÿ≤ MD ÿ®Ÿæÿ±ÿ≥€åÿØ (ŸÖÿ´ÿßŸÑ: MDÿå Ÿà€åÿØ€åŸà ÿ±ÿß Ÿæÿ±ÿØÿßÿ≤ÿ¥ ⁄©ŸÜ €åÿß ŸÖŸÜ ⁄©ÿ¨ÿß Ÿáÿ≥ÿ™ŸÖ)",
                process: "Ÿæÿ±ÿØÿßÿ≤ÿ¥",
                outputSection: "ÿÆÿ±Ÿàÿ¨€å"
            }
        };
        function updateLanguage() {
            const lang = document.getElementById('language').value;
            document.querySelector('h1').innerText = translations[lang].title;
            document.querySelector('p.text-xl').innerText = translations[lang].description;
            document.querySelector('label[for="user_type"]').innerText = translations[lang].userTypeLabel;
            document.querySelector('option[value="blind"]').innerText = translations[lang].userTypeBlind;
            document.querySelector('option[value="deaf"]').innerText = translations[lang].userTypeDeaf;
            document.querySelector('label[for="language"]').innerText = translations[lang].languageLabel;
            document.querySelector('h3[aria-label="Target Coordinates Section"]').innerText = translations[lang].targetCoords;
            document.getElementById('object_lat').placeholder = translations[lang].targetLat;
            document.getElementById('object_lon').placeholder = translations[lang].targetLon;
            document.querySelector('h3[aria-label="User Coordinates Section"]').innerText = translations[lang].userCoords;
            document.getElementById('user_lat').placeholder = translations[lang].userLat;
            document.getElementById('user_lon').placeholder = translations[lang].userLon;
            document.querySelector('h3[aria-label="Input Section"]').innerText = translations[lang].mediaInput;
            document.querySelector('h3[aria-label="Audio Input Section"]').innerText = translations[lang].audioInput;
            document.getElementById('listen-btn').innerText = translations[lang].startListening;
            document.querySelector('h3[aria-label="Query Section"]').innerText = translations[lang].querySection;
            document.getElementById('query').placeholder = translations[lang].queryPlaceholder;
            document.getElementById('process-btn').innerText = translations[lang].process;
            document.querySelector('h3[aria-label="Output Section"]').innerText = translations[lang].outputSection;
        }
        document.getElementById('language').addEventListener('change', updateLanguage);
        updateLanguage();
        function getUserLocation() {
            try {
                if (navigator.geolocation) {
                    navigator.geolocation.getCurrentPosition((position) => {
                        document.getElementById('user_lat').value = position.coords.latitude;
                        document.getElementById('user_lon').value = position.coords.longitude;
                        output.innerText = document.getElementById('language').value === 'fa' ? 'ŸÖÿÆÿ™ÿµÿßÿ™ GPS ÿØÿ±€åÿßŸÅÿ™ ÿ¥ÿØ.' : 'GPS coordinates acquired.';
                        if (document.getElementById('user_type').value === 'blind') speakOnServer(output.innerText);
                    }, (error) => {
                        output.innerText = document.getElementById('language').value === 'fa' ? 'ÿØÿ≥ÿ™ÿ±ÿ≥€å ÿ®Ÿá GPS ÿ±ÿØ ÿ¥ÿØ. ŸÑÿ∑ŸÅÿßŸã ŸÖÿÆÿ™ÿµÿßÿ™ ÿ±ÿß ÿØÿ≥ÿ™€å Ÿàÿßÿ±ÿØ ⁄©ŸÜ€åÿØ.' : 'GPS access denied. Please enter coordinates manually.';
                        if (document.getElementById('user_type').value === 'blind') speakOnServer(output.innerText);
                        console.error('GPS error: ' + error.message);
                    });
                } else {
                    output.innerText = document.getElementById('language').value === 'fa' ? 'ŸÖÿ±Ÿàÿ±⁄Øÿ± ÿßÿ≤ ŸÖŸàŸÇÿπ€åÿ™‚Äå€åÿßÿ®€å Ÿæÿ¥ÿ™€åÿ®ÿßŸÜ€å ŸÜŸÖ€å‚Äå⁄©ŸÜÿØ.' : 'Geolocation not supported by browser.';
                }
            } catch (e) {
                output.innerText = document.getElementById('language').value === 'fa' ? `ÿÆÿ∑ÿß€å ŸÖŸàŸÇÿπ€åÿ™‚Äå€åÿßÿ®€å: ${e.message}` : `Location error: ${e.message}`;
            }
        }
        getUserLocation();
        function updateButtonStyles() {
            console.log('updateButtonStyles called');
            const userType = document.getElementById('user_type').value;
            const buttons = document.querySelectorAll('button');
            buttons.forEach(btn => {
                btn.classList.toggle('blind-mode', userType === 'blind');
                btn.classList.toggle('deaf-mode', userType === 'deaf');
            });
        }
        async function speakOnServer(message) {
            try {
                const response = await fetch('/generate_audio', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        message: message,
                        language: document.getElementById('language').value
                    })
                });
                const result = await response.json();
                if (result.audio_url && document.getElementById('user_type').value === 'blind') {
                    audioOutput.src = result.audio_url;
                    audioOutput.play();
                    navigator.vibrate([200]);
                }
            } catch (e) {
                console.error('Server speech error:', e);
                output.innerText = document.getElementById('language').value === 'fa' ? `ÿÆÿ∑ÿß€å ÿ™ŸàŸÑ€åÿØ ÿµŸàÿ™: ${e}` : `Speech generation error: ${e}`;
                queryInput.value = output.innerText;
            }
        }
        async function checkMicPermission() {
            try {
                const permission = await navigator.permissions.query({ name: 'microphone' });
                if (permission.state === 'denied') {
                    output.innerText = document.getElementById('language').value === 'fa' ? 'ÿØÿ≥ÿ™ÿ±ÿ≥€å ÿ®Ÿá ŸÖ€å⁄©ÿ±ŸàŸÅŸàŸÜ ÿ±ÿØ ÿ¥ÿØ. ŸÑÿ∑ŸÅÿßŸã ÿ¢ŸÜ ÿ±ÿß ÿØÿ± ÿ™ŸÜÿ∏€åŸÖÿßÿ™ ŸÖÿ±Ÿàÿ±⁄Øÿ± ŸÅÿπÿßŸÑ ⁄©ŸÜ€åÿØ.' : 'Microphone access denied. Please enable it in browser settings.';
                    queryInput.value = output.innerText;
                    await speakOnServer(output.innerText);
                    return false;
                }
                return true;
            } catch (e) {
                output.innerText = document.getElementById('language').value === 'fa' ? `ÿÆÿ∑ÿß€å ÿ®ÿ±ÿ±ÿ≥€å ŸÖÿ¨Ÿàÿ≤ ŸÖ€å⁄©ÿ±ŸàŸÅŸàŸÜ: ${e}` : `Microphone permission check error: ${e}`;
                queryInput.value = output.innerText;
                await speakOnServer(output.innerText);
                return false;
            }
        }
        async function updateUserMode() {
            try {
                const userType = document.getElementById('user_type').value;
                document.body.className = userType === 'blind' ? 'blind-mode' : 'deaf-mode';
                output.className = `output-container p-6 rounded-lg ${userType === 'blind' ? 'blind-mode' : 'deaf-mode bg-gray-50'}`;
                updateButtonStyles();
                const message = userType === 'blind' ?
                    (document.getElementById('language').value === 'fa' ? 'ÿ≠ÿßŸÑÿ™ ⁄©ÿßÿ±ÿ®ÿ± ÿ®Ÿá ŸÜÿßÿ®€åŸÜÿß ÿ™ŸÜÿ∏€åŸÖ ÿ¥ÿØ. ÿ®ÿ±ÿß€å ŸÅÿπÿßŸÑ‚Äåÿ≥ÿßÿ≤€å ÿ®⁄ØŸà€å€åÿØ "MD".' : 'User mode set to blind. Say "MD" to activate.') :
                    (document.getElementById('language').value === 'fa' ? 'ÿ≠ÿßŸÑÿ™ ⁄©ÿßÿ±ÿ®ÿ± ÿ®Ÿá ŸÜÿßÿ¥ŸÜŸàÿß ÿ™ŸÜÿ∏€åŸÖ ÿ¥ÿØ. ÿÆÿ±Ÿàÿ¨€å‚ÄåŸáÿß ÿ®Ÿá‚ÄåÿµŸàÿ±ÿ™ ÿ®ÿµÿ±€å ŸÜŸÖÿß€åÿ¥ ÿØÿßÿØŸá ŸÖ€å‚Äåÿ¥ŸàŸÜÿØ.' : 'User mode set to deaf. Outputs are displayed visually.');
                output.innerText = message;
                queryInput.value = message;
                if (userType === 'blind') {
                    audioOutput.style.display = 'block';
                    output.style.fontSize = '2rem';
                    processedImage.style.display = 'none';
                    processedVideo.style.display = 'none';
                    await speakOnServer(message);
                    navigator.vibrate([200, 100, 200]);
                } else {
                    audioOutput.style.display = 'none';
                    output.style.fontSize = '1.125rem';
                    processedImage.style.display = 'block';
                    processedVideo.style.display = 'block';
                    output.classList.add('visual-alert');
                }
            } catch (e) {
                output.innerText = document.getElementById('language').value === 'fa' ? `ÿÆÿ∑ÿß€å ÿ™ÿ∫€å€åÿ± ÿ≠ÿßŸÑÿ™ ⁄©ÿßÿ±ÿ®ÿ±: ${e}` : `User mode change error: ${e}`;
                queryInput.value = output.innerText;
                await speakOnServer(output.innerText);
            }
        }
        async function startListening() {
            try {
                if (!(await checkMicPermission())) return;
                const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
                recognition.lang = document.getElementById('language').value === 'fa' ? 'fa-IR' : 'en-US';
                recognition.onstart = () => {
                    listening = true;
                    recordingStatus.classList.add('recording');
                    recordingStatus.innerText = document.getElementById('language').value === 'fa' ? 'ÿØÿ± ÿ≠ÿßŸÑ ⁄ØŸàÿ¥ ÿØÿßÿØŸÜ...' : 'Listening...';
                };
                recognition.onresult = async (event) => {
                    const transcript = event.results[0][0].transcript;
                    queryInput.value = transcript;
                    listening = false;
                    recordingStatus.classList.remove('recording');
                    recordingStatus.innerText = '';
                    if (transcript.toLowerCase().includes('process video') || transcript.toLowerCase().includes('Ÿæÿ±ÿØÿßÿ≤ÿ¥ Ÿà€åÿØ€åŸà') ||
                        transcript.toLowerCase().includes('where am i') || transcript.toLowerCase().includes('ŸÖŸÜ ⁄©ÿ¨ÿß Ÿáÿ≥ÿ™ŸÖ')) {
                        await processInput();
                    }
                    if (document.getElementById('user_type').value === 'blind') {
                        await speakOnServer(document.getElementById('language').value === 'fa' ? 'ŸÖÿ™ŸÜ ÿµŸàÿ™€å ÿØÿ± ŸÅ€åŸÑÿØ Ÿàÿ±ŸàÿØ€å ŸÇÿ±ÿßÿ± ⁄Øÿ±ŸÅÿ™.' : 'Voice input placed in query field.');
                        navigator.vibrate([200]);
                    } else if (document.getElementById('user_type').value === 'deaf') {
                        recordingStatus.innerText = 'üó£Ô∏è Input received!';
                        recordingStatus.classList.add('visual-alert');
                        setTimeout(() => recordingStatus.classList.remove('visual-alert'), 2000);
                    }
                };
                recognition.onend = () => {
                    listening = false;
                    recordingStatus.classList.remove('recording');
                    recordingStatus.innerText = '';
                };
                recognition.onerror = async (event) => {
                    output.innerText = document.getElementById('language').value === 'fa' ? `ÿÆÿ∑ÿß€å ÿ™ÿ¥ÿÆ€åÿµ ÿµŸàÿ™: ${event.error}` : `Speech recognition error: ${event.error}`;
                    queryInput.value = output.innerText;
                    if (document.getElementById('user_type').value === 'blind') await speakOnServer(output.innerText);
                };
                recognition.start();
            } catch (e) {
                output.innerText = document.getElementById('language').value === 'fa' ? `ÿÆÿ∑ÿß€å ÿ¥ÿ±Ÿàÿπ ⁄ØŸàÿ¥ ÿØÿßÿØŸÜ: ${e}` : `Listening error: ${e}`;
                queryInput.value = output.innerText;
                if (document.getElementById('user_type').value === 'blind') await speakOnServer(output.innerText);
            }
        }
        async function processInput(formData = new FormData()) {
            try {
                formData.append('user_type', document.getElementById('user_type').value);
                formData.append('language', document.getElementById('language').value);
                formData.append('query', document.getElementById('query').value);
                formData.append('object_lat', document.getElementById('object_lat').value || 35.1357);
                formData.append('object_lon', document.getElementById('object_lon').value || -121.8269);
                formData.append('user_lat', document.getElementById('user_lat').value || '');
                formData.append('user_lon', document.getElementById('user_lon').value || '');
                if (document.getElementById('media_input').files[0]) {
                    formData.append('media', document.getElementById('media_input').files[0]);
                }
                output.innerText = document.getElementById('language').value === 'fa' ? 'ÿØÿ± ÿ≠ÿßŸÑ Ÿæÿ±ÿØÿßÿ≤ÿ¥...' : 'Processing...';
                queryInput.value = output.innerText;
                const response = await fetch('/process_input', {
                    method: 'POST',
                    body: formData
                });
                const result = await response.json();
                console.log('Fetch response:', result);
                if (result.error) {
                    output.innerText = result.error;
                    queryInput.value = result.error;
                    processedImage.style.display = 'none';
                    processedVideo.style.display = 'none';
                    if (document.getElementById('user_type').value === 'blind') {
                        await speakOnServer(result.error);
                        navigator.vibrate([500]);
                    } else if (document.getElementById('user_type').value === 'deaf') {
                        output.classList.add('visual-alert');
                        setTimeout(() => output.classList.remove('visual-alert'), 2000);
                    }
                } else {
                    output.innerHTML = result.visual_output;
                    queryInput.value = result.description;
                    document.getElementById('history-log').value += `\n${result.description}`;
                    if (result.audio_url && document.getElementById('user_type').value === 'blind') {
                        audioOutput.src = result.audio_url;
                        audioOutput.play();
                    }
                    if (result.saved_file_path && document.getElementById('user_type').value === 'deaf') {
                        output.innerHTML += `<br><a href="${result.saved_file_url}" download>ÿØÿßŸÜŸÑŸàÿØ ŸÅÿß€åŸÑ Ÿæÿ±ÿØÿßÿ≤ÿ¥‚Äåÿ¥ÿØŸá</a>`;
                        if (result.saved_file_path.endsWith('.mp4')) {
                            processedVideo.src = `data:video/mp4;base64,${result.file_base64}`;
                            processedVideo.style.display = 'block';
                            processedImage.style.display = 'none';
                        } else {
                            processedImage.src = `data:image/jpeg;base64,${result.file_base64}`;
                            processedImage.style.display = 'block';
                            processedVideo.style.display = 'none';
                        }
                    } else {
                        processedImage.style.display = 'none';
                        processedVideo.style.display = 'none';
                    }
                }
            } catch (e) {
                output.innerText = document.getElementById('language').value === 'fa' ? `ÿÆÿ∑ÿß: ${e}` : `Error: ${e}`;
                queryInput.value = output.innerText;
                processedImage.style.display = 'none';
                processedVideo.style.display = 'none';
                if (document.getElementById('user_type').value === 'blind') {
                    await speakOnServer(output.innerText);
                    navigator.vibrate([500]);
                } else if (document.getElementById('user_type').value === 'deaf') {
                    output.classList.add('visual-alert');
                    setTimeout(() => output.classList.remove('visual-alert'), 2000);
                }
                console.error('Fetch error:', e);
            }
        }
        document.addEventListener('keydown', async (e) => {
            try {
                if (e.key === 'Enter' && document.activeElement.id === 'query') await processInput();
                if (e.key === 'l' && !listening) await startListening();
            } catch (e) {
                output.innerText = document.getElementById('language').value === 'fa' ? `ÿÆÿ∑ÿß€å ⁄©ŸÑ€åÿØ: ${e}` : `Key error: ${e}`;
                queryInput.value = output.innerText;
                if (document.getElementById('user_type').value === 'blind') await speakOnServer(output.innerText);
            }
        });
        if ('serviceWorker' in navigator) {
            navigator.serviceWorker.register('/service-worker.js').then(() => {
                console.log('Service Worker registered');
            }).catch(e => console.error('Service Worker registration failed:', e));
        }
        updateUserMode();
    </script>
</body>
</html>
"""

service_worker_content = """
self.addEventListener('install', event => {
    console.log('Service Worker installing');
    event.waitUntil(
        caches.open('md-visionaid-cache-v1').then(cache => {
            return cache.addAll([
                '/',
                '/manifest.json'
            ]);
        })
    );
});
self.addEventListener('fetch', event => {
    console.log('Service Worker fetching:', event.request.url);
    event.respondWith(
        caches.match(event.request).then(response => {
            return response || fetch(event.request);
        })
    );
});
"""

manifest_content = """
{
    "name": "MD: VisionAid+DeafAid",
    "short_name": "MD",
    "description": "An AI-powered assistive tool for blind and deaf users with video and object detection",
    "start_url": "/",
    "display": "standalone",
    "background_color": "#020202",
    "theme_color": "#1d4ed8",
    "icons": []
}
"""

def calculate_distance(lat1, lon1, lat2, lon2):
    try:
        R = 6371008
        lat1_rad = math.radians(float(lat1))
        lon1_rad = math.radians(float(lon1))
        lat2_rad = math.radians(float(lat2))
        lon2_rad = math.radians(float(lon2))
        dlat = lat2_rad - lat1_rad
        dlon = lon2_rad - lon1_rad
        a = math.sin(dlat / 2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon / 2)**2
        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
        distance = R * c
        bearing = math.atan2(math.sin(dlon)*math.cos(lat2_rad), math.cos(lat1_rad)*math.sin(lat2_rad)-math.sin(lat1_rad)*math.cos(lat2_rad)*math.cos(dlon))
        bearing = math.degrees(bearing)
        bearing = (bearing + 360) % 360
        directions = ['North', 'Northeast', 'East', 'Southeast', 'South', 'Southwest', 'West', 'Northwest']
        dir_idx = round(bearing / 45) % 8
        direction = directions[dir_idx]
        return distance, direction
    except Exception as e:
        print(f"Distance calculation error: {str(e)}")
        return None, None

def analyze_input(media, audio_path, language='en', query=None, user_type='blind', user_lat=None, user_lon=None):
    description = ""
    saved_file_path = None
    base64_file = None
    objects = []
    commands = {
        'en': {
            'save image': 'Image saved for later use.',
            'save video': 'Video frame saved for later use.',
            'process video': 'Processing video.',
            'hello': 'Hello! How can MD assist you today?',
            'where am i': 'Describing your current location.',
            'describe scene': 'Describing the current scene.',
            'find object': 'Searching for specified object.',
            'navigate to': 'Providing navigation directions.',
            'help': 'Listing available commands.',
            'read text': 'Reading text from image.',
            'detect colors': 'Detecting dominant colors.',
            'generate subtitles': 'Generating subtitles for video.',
            'switch language': 'Switching language.',
            'enable alerts': 'Enabling visual alerts.',
            'process image': 'Processing image.',
            'get distance': 'Estimating distance to objects.',
            'detect objects': 'Detecting objects in media.',
            'get coordinates': 'Retrieving current GPS coordinates.',
            'clean files': 'Cleaning old files.'
        },
        'fa': {
            'ÿ∞ÿÆ€åÿ±Ÿá ÿ™ÿµŸà€åÿ±': 'ÿ™ÿµŸà€åÿ± ÿ®ÿ±ÿß€å ÿßÿ≥ÿ™ŸÅÿßÿØŸá ÿ®ÿπÿØ€å ÿ∞ÿÆ€åÿ±Ÿá ÿ¥ÿØ.',
            'ÿ∞ÿÆ€åÿ±Ÿá Ÿà€åÿØ€åŸà': 'ŸÅÿ±€åŸÖ Ÿà€åÿØ€åŸà€å€å ÿ®ÿ±ÿß€å ÿßÿ≥ÿ™ŸÅÿßÿØŸá ÿ®ÿπÿØ€å ÿ∞ÿÆ€åÿ±Ÿá ÿ¥ÿØ.',
            'Ÿæÿ±ÿØÿßÿ≤ÿ¥ Ÿà€åÿØ€åŸà': 'Ÿæÿ±ÿØÿßÿ≤ÿ¥ Ÿà€åÿØ€åŸà.',
            'ÿ≥ŸÑÿßŸÖ': 'ÿ≥ŸÑÿßŸÖ! MD ⁄Ü⁄ØŸàŸÜŸá ŸÖ€å‚Äåÿ™ŸàÿßŸÜÿØ ÿ®Ÿá ÿ¥ŸÖÿß ⁄©ŸÖ⁄© ⁄©ŸÜÿØÿü',
            'ŸÖŸÜ ⁄©ÿ¨ÿß Ÿáÿ≥ÿ™ŸÖ': 'ÿ™Ÿàÿµ€åŸÅ ŸÖ⁄©ÿßŸÜ ŸÅÿπŸÑ€å ÿ¥ŸÖÿß.',
            'ÿ™Ÿàÿµ€åŸÅ ÿµÿ≠ŸÜŸá': 'ÿ™Ÿàÿµ€åŸÅ ÿµÿ≠ŸÜŸá ŸÅÿπŸÑ€å.',
            'Ÿæ€åÿØÿß ⁄©ÿ±ÿØŸÜ ÿ¥€å': 'ÿ¨ÿ≥ÿ™ÿ¨Ÿà€å ÿ¥€å ŸÖÿ¥ÿÆÿµ‚Äåÿ¥ÿØŸá.',
            'ŸÜÿßŸàÿ®ÿ±€å ÿ®Ÿá': 'ÿßÿ±ÿßÿ¶Ÿá ÿ¨Ÿáÿ™‚ÄåŸáÿß€å ŸÜÿßŸàÿ®ÿ±€å.',
            '⁄©ŸÖ⁄©': 'ŸÑ€åÿ≥ÿ™ ÿØÿ≥ÿ™Ÿàÿ±ÿßÿ™ ŸÖŸàÿ¨ŸàÿØ.',
            'ÿÆŸàÿßŸÜÿØŸÜ ŸÖÿ™ŸÜ': 'ÿÆŸàÿßŸÜÿØŸÜ ŸÖÿ™ŸÜ ÿßÿ≤ ÿ™ÿµŸà€åÿ±.',
            'ÿ™ÿ¥ÿÆ€åÿµ ÿ±ŸÜ⁄Ø‚ÄåŸáÿß': 'ÿ™ÿ¥ÿÆ€åÿµ ÿ±ŸÜ⁄Ø‚ÄåŸáÿß€å ÿ∫ÿßŸÑÿ®.',
            'ÿ™ŸàŸÑ€åÿØ ÿ≤€åÿ±ŸÜŸà€åÿ≥': 'ÿ™ŸàŸÑ€åÿØ ÿ≤€åÿ±ŸÜŸà€åÿ≥ ÿ®ÿ±ÿß€å Ÿà€åÿØ€åŸà.',
            'ÿ™ÿ∫€å€åÿ± ÿ≤ÿ®ÿßŸÜ': 'ÿ™ÿ∫€å€åÿ± ÿ≤ÿ®ÿßŸÜ.',
            'ŸÅÿπÿßŸÑ ⁄©ÿ±ÿØŸÜ Ÿáÿ¥ÿØÿßÿ±Ÿáÿß': 'ŸÅÿπÿßŸÑ ⁄©ÿ±ÿØŸÜ Ÿáÿ¥ÿØÿßÿ±Ÿáÿß€å ÿ®ÿµÿ±€å.',
            'Ÿæÿ±ÿØÿßÿ≤ÿ¥ ÿ™ÿµŸà€åÿ±': 'Ÿæÿ±ÿØÿßÿ≤ÿ¥ ÿ™ÿµŸà€åÿ±.',
            'ÿØÿ±€åÿßŸÅÿ™ ŸÅÿßÿµŸÑŸá': 'ÿ™ÿÆŸÖ€åŸÜ ŸÅÿßÿµŸÑŸá ÿ™ÿß ÿßÿ¥€åÿß.',
            'ÿ™ÿ¥ÿÆ€åÿµ ÿßÿ¥€åÿß': 'ÿ™ÿ¥ÿÆ€åÿµ ÿßÿ¥€åÿß ÿØÿ± ÿ±ÿ≥ÿßŸÜŸá.',
            'ÿØÿ±€åÿßŸÅÿ™ ŸÖÿÆÿ™ÿµÿßÿ™': 'ÿØÿ±€åÿßŸÅÿ™ ŸÖÿÆÿ™ÿµÿßÿ™ GPS ŸÅÿπŸÑ€å.',
            'Ÿæÿß⁄© ⁄©ÿ±ÿØŸÜ ŸÅÿß€åŸÑ‚ÄåŸáÿß': 'Ÿæÿß⁄© ⁄©ÿ±ÿØŸÜ ŸÅÿß€åŸÑ‚ÄåŸáÿß€å ŸÇÿØ€åŸÖ€å.'
        }
    }
    try:
        if query and gemma_model and gemma_tokenizer:
            with torch.no_grad():
                prompt = f"Interpret this user query for an assistive AI in {language}, extract main command (e.g., 'process video'), and suggest response structure: {query}"
                inputs = gemma_tokenizer(prompt, return_tensors="pt").to(device)
                output = gemma_model.generate(**inputs, max_new_tokens=50, temperature=0.7, do_sample=True)
                command = gemma_tokenizer.decode(output[0][len(inputs.input_ids[0]):], skip_special_tokens=True).lower()
        else:
            query = query.lower() if query else ""
            command = query
        latitude = float(user_lat) if user_lat and user_lat != '' else None
        longitude = float(user_lon) if user_lon and user_lon != '' else None
        if latitude is None or longitude is None:
            try:
                g = geocoder.ip('me')
                if g.ok:
                    latitude, longitude = g.latlng
                else:
                    latitude, longitude = 35.6895, 139.6917
                    description += "Unable to retrieve GPS coordinates. Using default location. " if language == 'en' else "ŸÜŸÖ€å‚Äåÿ™ŸàÿßŸÜ ŸÖÿÆÿ™ÿµÿßÿ™ GPS ÿ±ÿß ÿØÿ±€åÿßŸÅÿ™ ⁄©ÿ±ÿØ. ÿßÿ≤ ŸÖ⁄©ÿßŸÜ Ÿæ€åÿ¥‚ÄåŸÅÿ±ÿ∂ ÿßÿ≥ÿ™ŸÅÿßÿØŸá ŸÖ€å‚Äåÿ¥ŸàÿØ. "
            except Exception as e:
                description += f"GPS retrieval error: {str(e)}. " if language == 'en' else f"ÿÆÿ∑ÿß€å ÿØÿ±€åÿßŸÅÿ™ GPS: {str(e)}. "
                print(f"Geocoder error: {str(e)}")
        if command in commands[language]:
            if gemma_model and gemma_tokenizer:
                with torch.no_grad():
                    prompt = f"Generate a natural response in {language} for this command in an assistive app: {command}"
                    inputs = gemma_tokenizer(prompt, return_tensors="pt").to(device)
                    output = gemma_model.generate(**inputs, max_new_tokens=50, temperature=0.7, do_sample=True)
                    description += gemma_tokenizer.decode(output[0][len(inputs.input_ids[0]):], skip_special_tokens=True) + " "
            else:
                description += commands[language][command] + " "
        if media:
            clean_old_files()
            media_type = media.content_type if hasattr(media, 'content_type') else 'image'
            if 'video' in media_type:
                os.makedirs('saved_files', exist_ok=True)
                video_path = f"saved_files/video_{int(time.time())}.mp4"
                media.save(video_path)
                video_description, saved_file_path, base64_file, objects = process_video(video_path, language, user_type)
                description += video_description
                if command in ['save video', 'ÿ∞ÿÆ€åÿ±Ÿá Ÿà€åÿØ€åŸà']:
                    description += f"Video saved at {saved_file_path}. " if language == 'en' else f"Ÿà€åÿØ€åŸà ÿØÿ± {saved_file_path} ÿ∞ÿÆ€åÿ±Ÿá ÿ¥ÿØ. "
                if os.path.exists(video_path):
                    os.remove(video_path)
            else:
                image = Image.open(media.stream if hasattr(media, 'stream') else BytesIO(media.read())).convert('RGB').resize((640, 480))
                image_description, saved_file_path, base64_file, objects = process_image_with_objects(image, language, user_type, query)
                description += image_description
                if command in ['save image', 'ÿ∞ÿÆ€åÿ±Ÿá ÿ™ÿµŸà€åÿ±']:
                    description += f"Image saved at {saved_file_path}. " if language == 'en' else f"ÿ™ÿµŸà€åÿ± ÿØÿ± {saved_file_path} ÿ∞ÿÆ€åÿ±Ÿá ÿ¥ÿØ. "
        if query and ("where am i" in command or (language == 'fa' and "ŸÖŸÜ ⁄©ÿ¨ÿß Ÿáÿ≥ÿ™ŸÖ" in command)):
            try:
                if latitude is None or longitude is None:
                    description += "GPS coordinates not provided. Please ensure location services are enabled or enter manually. " if language == 'en' else "ŸÖÿÆÿ™ÿµÿßÿ™ GPS ÿßÿ±ÿßÿ¶Ÿá ŸÜÿ¥ÿØŸá ÿßÿ≥ÿ™. ŸÑÿ∑ŸÅÿßŸã ÿÆÿØŸÖÿßÿ™ ŸÖŸàŸÇÿπ€åÿ™‚Äå€åÿßÿ®€å ÿ±ÿß ŸÅÿπÿßŸÑ ⁄©ŸÜ€åÿØ €åÿß ÿØÿ≥ÿ™€å Ÿàÿßÿ±ÿØ ⁄©ŸÜ€åÿØ. "
                else:
                    location_description = (
                        f"Your approximate coordinates are Latitude {latitude:.4f}, Longitude {longitude:.4f}. "
                        if language == 'en' else
                        f"ŸÖÿÆÿ™ÿµÿßÿ™ ÿ™ŸÇÿ±€åÿ®€å ÿ¥ŸÖÿß ÿπÿ±ÿ∂ ÿ¨ÿ∫ÿ±ÿßŸÅ€åÿß€å€å {latitude:.4f}ÿå ÿ∑ŸàŸÑ ÿ¨ÿ∫ÿ±ÿßŸÅ€åÿß€å€å {longitude:.4f} ÿßÿ≥ÿ™."
                    )
                    if objects:
                        location_description += f"Detected {len(objects)} objects: " if language == 'en' else f"{len(objects)} ÿ¥€åÿ° ÿ¥ŸÜÿßÿ≥ÿß€å€å ÿ¥ÿØ: "
                        object_counts = Counter([obj['name'] for obj in objects])
                        location_description += ', '.join([f"{count} {obj}" if count > 1 else obj for obj, count in object_counts.items()]) + ". "
                    description += location_description
                    try:
                        object_lat = float(request.form.get('object_lat', latitude or 35.1357))
                        object_lon = float(request.form.get('object_lon', longitude or -121.8269))
                        distance, direction = calculate_distance(latitude or 35.6895, longitude or 139.6917, object_lat, object_lon)
                        steps = int(distance / 0.75) if distance is not None else 0
                        if distance is not None:
                            gps_data = (
                                f"You are approximately {steps} steps away from the target at Latitude {object_lat:.4f}, Longitude {object_lon:.4f}, heading {direction}. "
                                if language == 'en' else
                                f"ÿ¥ŸÖÿß ÿ™ŸÇÿ±€åÿ®ÿßŸã {steps} ŸÇÿØŸÖ ÿßÿ≤ ŸáÿØŸÅ ÿØÿ± ÿπÿ±ÿ∂ ÿ¨ÿ∫ÿ±ÿßŸÅ€åÿß€å€å {object_lat:.4f}ÿå ÿ∑ŸàŸÑ ÿ¨ÿ∫ÿ±ÿßŸÅ€åÿß€å€å {object_lon:.4f} ÿ®Ÿá ÿ≥ŸÖÿ™ {direction} ŸÅÿßÿµŸÑŸá ÿØÿßÿ±€åÿØ."
                            )
                            description += gps_data
                        else:
                            description += "Unable to calculate distance to target. " if language == 'en' else "ŸÜŸÖ€å‚Äåÿ™ŸàÿßŸÜ ŸÅÿßÿµŸÑŸá ÿ™ÿß ŸáÿØŸÅ ÿ±ÿß ŸÖÿ≠ÿßÿ≥ÿ®Ÿá ⁄©ÿ±ÿØ. "
                    except Exception as e:
                        description += f"GPS calculation error: {str(e)}. " if language == 'en' else f"ÿÆÿ∑ÿß€å ŸÖÿ≠ÿßÿ≥ÿ®Ÿá GPS: {str(e)}. "
                        print(f"GPS calculation error: {str(e)}")
            except Exception as e:
                description += f"Location processing error: {str(e)}. " if language == 'en' else f"ÿÆÿ∑ÿß€å Ÿæÿ±ÿØÿßÿ≤ÿ¥ ŸÖ⁄©ÿßŸÜ: {str(e)}. "
                print(f"Location processing error: {str(e)}")
        if query and gemma_model and gemma_tokenizer:
            try:
                target_query = query.strip().lower()
                if target_query.startswith("md"):
                    target_query = target_query[2:].strip()
                if "greate" in target_query:
                    target_query = target_query.replace("greate", "great")
                if not any(key in target_query for key in commands[language]):
                    prompt = (
                        f"Provide a clear and concise response in one sentence for {'blind users' if user_type == 'blind' else 'deaf users'} in {language}, interpreting the query as a request for information about the environment or objects, avoiding repetition: {target_query}"
                        if language == 'en' else
                        f"€å⁄© Ÿæÿßÿ≥ÿÆ Ÿàÿßÿ∂ÿ≠ Ÿà ŸÖÿÆÿ™ÿµÿ± ÿØÿ± €å⁄© ÿ¨ŸÖŸÑŸá ÿ®ÿ±ÿß€å {'⁄©ÿßÿ±ÿ®ÿ±ÿßŸÜ ŸÜÿßÿ®€åŸÜÿß' if user_type == 'blind' else '⁄©ÿßÿ±ÿ®ÿ±ÿßŸÜ ŸÜÿßÿ¥ŸÜŸàÿß'} ÿ®Ÿá {language} ÿßÿ±ÿßÿ¶Ÿá ÿØŸá€åÿØÿå ÿ®ÿß ÿ™ŸÅÿ≥€åÿ± Ÿæÿ±ÿ≥‚ÄåŸàÿ¨Ÿà ÿ®Ÿá‚ÄåÿπŸÜŸàÿßŸÜ ÿØÿ±ÿÆŸàÿßÿ≥ÿ™ ÿßÿ∑ŸÑÿßÿπÿßÿ™ ÿØÿ±ÿ®ÿßÿ±Ÿá ŸÖÿ≠€åÿ∑ €åÿß ÿßÿ¥€åÿßÿå ÿ®ÿØŸàŸÜ ÿ™⁄©ÿ±ÿßÿ±: {target_query}"
                    )
                    inputs = gemma_tokenizer(prompt, return_tensors="pt").to(device)
                    output = gemma_model.generate(**inputs, max_new_tokens=50, temperature=0.7, do_sample=True, no_repeat_ngram_size=3)
                    query_response = gemma_tokenizer.decode(output[0][len(inputs.input_ids[0]):], skip_special_tokens=True)
                    description += f"Response: {query_response}. " if language == 'en' else f"Ÿæÿßÿ≥ÿÆ: {query_response}. "
            except Exception as e:
                description += f"Query processing error: {str(e)}. " if language == 'en' else f"ÿÆÿ∑ÿß€å Ÿæÿ±ÿØÿßÿ≤ÿ¥ Ÿæÿ±ÿ≥‚ÄåŸàÿ¨Ÿà: {str(e)}. "
                print(f"Query processing error: {str(e)}")
        audio_path = None
        visual_output = description
        if user_type == 'blind':
            audio_path = f"audio/output_{int(time.time())}.mp3"
            os.makedirs('audio', exist_ok=True)
            try:
                tts = gTTS(text=description + (" Processing complete." if language == 'en' else " Ÿæÿ±ÿØÿßÿ≤ÿ¥ Ÿæÿß€åÿßŸÜ €åÿßŸÅÿ™."), lang=language, slow=False, tld='co.uk')
                tts.save(audio_path)
            except Exception as e:
                print(f"gTTS error: {str(e)}")
                audio_path = None
        elif user_type == 'deaf':
            visual_output = f"<div class='bg-blue-100 p-6 rounded-xl shadow-lg text-lg leading-relaxed'>{description}</div>"
        return description, audio_path, visual_output, objects, False, False, "", None, saved_file_path, base64_file
    except Exception as e:
        print(f"Error in analyze_input: {str(e)}")
        error_msg = f"Error: {str(e)}" if language == 'en' else f"ÿÆÿ∑ÿß: {str(e)}"
        return error_msg, None, error_msg, [], False, False, "", None, None, None

@app.route('/')
def index():
    return render_template_string(html_content)

@app.route('/generate_audio', methods=['POST'])
def generate_audio():
    try:
        data = request.get_json()
        if not data or 'message' not in data:
            return jsonify({'error': 'Invalid or missing JSON data'}), 400
        message = data.get('message', '')
        language = data.get('language', 'en')
        audio_path = generate_system_audio(message, language)
        if audio_path:
            return jsonify({'audio_url': f'/get_audio/{os.path.basename(audio_path)}'})
        else:
            return jsonify({'error': 'Failed to generate audio'}), 500
    except Exception as e:
        print(f"Generate audio error: {str(e)}")
        return jsonify({'error': f"Generate audio error: {str(e)}"}), 500

@app.route('/process_input', methods=['POST'])
def process_input():
    start_time = time.time()
    language = request.form.get('language', 'en')
    user_type = request.form.get('user_type', 'blind')
    query = request.form.get('query', None)
    user_lat = request.form.get('user_lat')
    user_lon = request.form.get('user_lon')
    try:
        media = None
        audio_path = None
        if 'audio' in request.files:
            audio_file = request.files['audio']
            audio_path = f"temp_audio_{int(time.time())}.webm"
            audio_file.save(audio_path)
            query = transcribe_audio(audio_path, language)
            if os.path.exists(audio_path):
                os.remove(audio_path)
        if 'media' in request.files:
            media = request.files['media']
        elif request.form.get('image_data'):
            data_url = request.form.get('image_data')
            header, encoded = data_url.split(',', 1)
            data = b64decode(encoded)
            media = BytesIO(data)
        description, audio_output, visual_output, objects, take_photo_triggered, warning, warning_message, warning_audio, saved_file_path, base64_file = analyze_input(
            media, audio_path, language, query, user_type, user_lat, user_lon
        )
        processing_time = time.time() - start_time
        response = {
            'description': description,
            'audio_url': f'/get_audio/{os.path.basename(audio_output)}' if audio_output else None,
            'visual_output': visual_output,
            'objects': objects,
            'processing_time': f"{processing_time:.2f} seconds",
            'take_photo_triggered': take_photo_triggered,
            'warning': warning,
            'warning_message': warning_message,
            'warning_audio_url': f'/get_audio/{os.path.basename(warning_audio)}' if warning_audio else None,
            'saved_file_path': saved_file_path,
            'saved_file_url': f'/get_file/{os.path.basename(saved_file_path)}' if saved_file_path else None,
            'file_base64': base64_file
        }
        return jsonify(response)
    except Exception as e:
        print(f"Process input error: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/get_audio/<filename>')
def get_audio(filename):
    try:
        return send_file(os.path.join('audio', filename), mimetype='audio/mp3')
    except Exception as e:
        print(f"Audio file error: {str(e)}")
        return jsonify({'error': f"Audio file error: {str(e)}"}), 500

@app.route('/get_file/<filename>')
def get_file(filename):
    try:
        if filename.endswith('.mp4'):
            return send_file(os.path.join('saved_files', filename), mimetype='video/mp4')
        else:
            return send_file(os.path.join('saved_files', filename), mimetype='image/jpeg')
    except Exception as e:
        print(f"File error: {str(e)}")
        return jsonify({'error': f"File error: {str(e)}"}), 500

ngrok.set_auth_token(os.getenv('NGROK_TOKEN', '2s9UYBDAhKCGblwUTV8PPf5AhJg_75PDRoniQ2hhzvtmoy8SN'))
def keep_alive():
    while True:
        time.sleep(3600)
threading.Thread(target=keep_alive, daemon=True).start()

if __name__ == '__main__':
    http_tunnel = ngrok.connect(5000, bind_tls=True)
    public_url = http_tunnel.public_url
    print(f"Flask app running at: {public_url}")
    app.run(port=5000)

import os
os.kill(os.getpid(), 9)
