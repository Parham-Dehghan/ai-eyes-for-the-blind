# -*- coding: utf-8 -*-
"""python ai_eyes_for_the_blind_google_gemma_3n_impact_challenge.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DrmJqmlnvTyKYffBps1bjPLs7lUb-LFU
"""

# -*- coding: utf-8 -*-
"""python ai_eyes_for_the_blind_google_gemma_3n_impact_challenge.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DrmJqmlnvTyKYffBps1bjPLs7lUb-LFU
"""

"""
The MIT License

Copyright 2025 Mohammad Parham Dehghan

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
"""

# -*- coding: utf-8 -*-
"""VisionAid+DeafAid App Optimized for Gemma Hackathon with Video and Object Detection for Colab"""

# Suppress warnings
import warnings
warnings.filterwarnings('ignore')

# Install prerequisites for opencv and geocoder
!apt update -q && apt install -y libopencv-dev python3-opencv -q

# Install tesseract OCR
!apt install tesseract-ocr -q

# Update torch to compatible version
!pip install -q torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1

# Reformed pip install to avoid dependency conflicts in Colab
!pip install -q numpy==1.26.4 ultralytics==8.2.58 flask==2.2.5 transformers==4.42.4 pillow==10.4.0 gTTS==2.3.2 speechrecognition==3.10.0 pycocotools==2.0.7 pyngrok==6.0.0 opencv-python==4.10.0.84 geocoder==1.38.1 flask-cors==4.0.1 pytesseract -q

# Install Doctr for OCR
!pip install -q python-doctr

from flask_cors import CORS
import os
import time
import math
import numpy as np
from flask import Flask, request, render_template_string, Response, jsonify, send_file
from transformers import AutoTokenizer, AutoModelForCausalLM, BlipProcessor, BlipForConditionalGeneration, pipeline
from PIL import Image
from io import BytesIO
from base64 import b64decode, b64encode
from gtts import gTTS
import torch
from torch.quantization import quantize_dynamic
import speech_recognition as sr
import cv2
import geocoder
import re
import threading
from collections import Counter
from ultralytics import YOLO
from pyngrok import ngrok
import glob
import pytesseract
from doctr.io import DocumentFile
from doctr.models import ocr_predictor
import io

# Resolve ngrok config warning by moving legacy file
os.system('mkdir -p /root/.config/ngrok')
os.system('mv /root/.ngrok2/ngrok.yml /root/.config/ngrok/ngrok.yml || true')

# Set HF token securely
os.environ["HF_TOKEN"] = "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"

# Flask app
app = Flask(__name__)
CORS(app)

# Device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Download YOLO model if not exist
if not os.path.exists('yolov8n.pt'):
    os.system('wget -q https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt')
    if not os.path.exists('yolov8n.pt'):
        print("Failed to download yolov8n.pt. Please download manually.")
else:
    print("yolov8n.pt already exists.")

def load_models():
    try:
        start_time = time.time()
        print("Loading BLIP processor and model...")
        blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
        blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device)
        print("Loading Gemma model and tokenizer...")
        gemma_model = AutoModelForCausalLM.from_pretrained("google/gemma-2b").to(device)
        gemma_tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b")
        print("Loading Whisper model...")
        whisper = pipeline("automatic-speech-recognition", model="openai/whisper-tiny")
        print("Loading YOLO model...")
        yolo_model = YOLO("yolov8n.pt").to(device)
        print("Loading MiDaS depth model...")
        midas = torch.hub.load("intel-isl/MiDaS", "MiDaS_small").to(device)
        midas_transforms = torch.hub.load("intel-isl/MiDaS", "transforms")
        midas_transform = midas_transforms.small_transform
        print(f"Model loading time: {time.time() - start_time:.2f} seconds")
        return blip_processor, blip_model, gemma_tokenizer, gemma_model, whisper, yolo_model, midas, midas_transform
    except Exception as e:
        print(f"Error loading models: {str(e)}")
        return None, None, None, None, None, None, None, None

# Call models
blip_processor, blip_model, gemma_tokenizer, gemma_model, whisper, yolo_model, midas, midas_transform = load_models()

def transcribe_audio(audio_path, language='en'):
    try:
        if whisper is None:
            return "Transcription model not loaded."
        result = whisper(audio_path, language='fa' if language == 'fa' else 'en')
        text = result['text'].strip().lower()
        if gemma_model and gemma_tokenizer:
            with torch.no_grad():
                prompt = f"Refine this transcribed speech for clarity in {language}: {text}"
                inputs = gemma_tokenizer(prompt, return_tensors="pt").to(device)
                output = gemma_model.generate(**inputs, max_new_tokens=50, temperature=0.7, do_sample=True)
                text = gemma_tokenizer.decode(output[0][len(inputs.input_ids[0]):], skip_special_tokens=True)
        return text
    except Exception as e:
        print(f"Audio transcription error: {str(e)}")
        return "Transcription failed. Please use text input."

def clean_old_files(directory='saved_files', max_age_seconds=600):
    try:
        current_time = time.time()
        for file_path in glob.glob(os.path.join(directory, '*')) + glob.glob('audio/*.mp3'):
            if os.path.getmtime(file_path) < current_time - max_age_seconds:
                os.remove(file_path)
                print(f"Deleted old file: {file_path}")
    except Exception as e:
        print(f"Error cleaning old files: {str(e)}")

def generate_system_audio(message, language='en'):
    audio_path = f"audio/system_{int(time.time())}.mp3"
    os.makedirs('audio', exist_ok=True)
    try:
        tts = gTTS(text=message, lang=language, slow=False, tld='co.uk')
        tts.save(audio_path)
        return audio_path
    except Exception as e:
        print(f"gTTS error: {str(e)}")
        return None

def estimate_distance(depth_map, bbox, focal_length=600, avg_step_length=0.75):
    try:
        x1, y1, x2, y2 = map(int, bbox)
        region_depth = depth_map[y1:y2, x1:x2]
        if region_depth.size == 0:
            return "Unknown"
        avg_depth = np.mean(region_depth)
        distance_m = focal_length / avg_depth if avg_depth > 0 else float('inf')
        steps = int(distance_m / avg_step_length) if distance_m != float('inf') else "Far away"
        return f"{steps} steps away"
    except Exception as e:
        return f"Distance error: {str(e)}"

def extract_text_from_image(image):
    try:
        # ذخیره تصویر موقت
        temp_path = 'temp_image.png'
        image.save(temp_path)

        # بارگذاری تصویر با Doctr
        doc = DocumentFile.from_images(temp_path)

        # مدل OCR (برای انگلیسی)
        model = ocr_predictor(det_arch='db_resnet50', reco_arch='crnn_vgg16_bn', pretrained=True)

        # شناسایی متن
        result = model(doc)
        text = result.render().strip()

        # پاک کردن فایل موقت
        os.remove(temp_path)

        return text or "No text detected."
    except Exception as e:
        return f"OCR error: {str(e)}"

def generate_subtitles(video_path, language='en'):
    try:
        cap = cv2.VideoCapture(video_path)
        audio_path = f"temp_audio_{int(time.time())}.wav"
        os.system(f"ffmpeg -i {video_path} -vn -acodec pcm_s16le -ar 16000 -ac 1 {audio_path} -y -loglevel quiet")
        subtitles = transcribe_audio(audio_path, language)
        if os.path.exists(audio_path):
            os.remove(audio_path)
        return subtitles
    except Exception as e:
        print(f"Subtitles generation error: {str(e)}")
        return "Subtitles generation failed."

def process_image_with_objects(image, language='en', user_type='blind', query=''):
    try:
        is_read_text = (('md read text' in query.lower() or 'md خواندن متن' in query.lower()) or ('read text' in query.lower() or 'خواندن متن' in query.lower()))

        if is_read_text:
            # فقط استخراج متن، بدون توصیف تصویر
            text = extract_text_from_image(image)
            description = f"Extracted text: {text} " if language == 'en' else f"متن استخراج‌شده: {text} "
            generate_system_audio(text, language)
            return description, None, None, []

        # پردازش عادی برای موارد دیگر
        if yolo_model is None:
            return "Object detection model not loaded.", None, None, []
        results = yolo_model.predict(image, imgsz=640, conf=0.6)
        boxes = results[0].boxes.xyxy.cpu().numpy()
        confidences = results[0].boxes.conf.cpu().numpy()
        class_names = [results[0].names[int(cls)] for cls in results[0].boxes.cls]
        detected_objects = [name for name, conf in zip(class_names, confidences) if conf > 0.6]
        object_counts = Counter(detected_objects)
        img_array = np.array(image)
        img_array = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
        objects = []
        for box, cls, conf in zip(boxes, results[0].boxes.cls, confidences):
            if conf > 0.6:
                x1, y1, x2, y2 = map(int, box)
                class_name = results[0].names[int(cls)]
                cv2.rectangle(img_array, (x1, y1), (x2, y2), (0, 0, 255), 2)
                cv2.putText(img_array, f"{class_name} ({conf:.2f})", (x1, y1 - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
                objects.append({
                    'name': class_name,
                    'x1': float(x1),
                    'y1': float(y1),
                    'x2': float(x2),
                    'y2': float(y2),
                    'new': True
                })
        if midas and midas_transform:
            img = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)
            input_batch = midas_transform(img).to(device)
            with torch.no_grad():
                depth_map = midas(input_batch).cpu().numpy().squeeze()
            depth_map = cv2.resize(depth_map, (img.shape[1], img.shape[0]))
        else:
            depth_map = None
        processed_image = Image.fromarray(cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB))
        os.makedirs('saved_files', exist_ok=True)
        saved_file_path = f"saved_files/processed_{int(time.time())}.jpg"
        processed_image.save(saved_file_path, quality=90)
        base64_file = b64encode(open(saved_file_path, 'rb').read()).decode('utf-8')
        description = ""
        if detected_objects:
            description += f"Detected {len(detected_objects)} objects: " if language == 'en' else f"{len(detected_objects)} شیء شناسایی شد: "
            description += ', '.join([f"{count} {obj}" if count > 1 else obj for obj, count in object_counts.items()]) + ". "
        for i, (box, cls, conf) in enumerate(zip(boxes, results[0].boxes.cls, confidences)):
            if conf > 0.6:
                dist = estimate_distance(depth_map, box, focal_length=600) if depth_map is not None else "Distance not estimated"
                objects[i]['distance'] = dist
                class_name = results[0].names[int(cls)]
                description += f"{class_name} is {dist}. " if language == 'en' else f"{class_name} در {dist} قرار دارد. "
        if blip_processor and blip_model:
            inputs = blip_processor(images=image, return_tensors="pt").to(device)
            caption = blip_model.generate(**inputs, max_length=100, num_beams=5, no_repeat_ngram_size=2)
            caption_text = blip_processor.decode(caption[0], skip_special_tokens=True)
            if len(caption_text.split()) < 3:
                caption_text = "A lively scene with various objects." if language == 'en' else "صحنه‌ای پرجنب‌وجوش با اشیای مختلف."
        else:
            caption_text = "Fallback: Scene with objects."
        if gemma_model and gemma_tokenizer:
            prompt = f"Refine this caption for {user_type} users in {language} to enhance accessibility: Describe key elements, spatial layout, estimated distances, and object interactions clearly and concisely, without repetition or unnecessary details: {caption_text}"
            inputs = gemma_tokenizer(prompt, return_tensors="pt").to(device)
            output = gemma_model.generate(**inputs, max_new_tokens=100, temperature=0.6, do_sample=True, no_repeat_ngram_size=3)
            caption_text = gemma_tokenizer.decode(output[0][len(inputs.input_ids[0]):], skip_special_tokens=True)
            description += f"Image description: {caption_text}. " if language == 'en' else f"توصیف تصویر: {caption_text}. "
        try:
            pixels = img_array.reshape(-1, 3)
            color_counts = Counter(map(tuple, pixels))
            dominant_colors = [f"rgb{t}" for t, count in color_counts.most_common(3)]
            description += f"Dominant colors: {', '.join(dominant_colors)}. " if language == 'en' else f"رنگ‌های غالب: {', '.join(dominant_colors)}. "
        except Exception as e:
            description += f"Color detection error: {str(e)}. " if language == 'en' else f"خطای تشخیص رنگ: {str(e)}. "
        return description, saved_file_path, base64_file, objects
    except Exception as e:
        print(f"Image processing error: {str(e)}")
        return f"Image processing error: {str(e)}", None, None, []

def process_video(video_path, language='en', user_type='blind'):
    try:
        if yolo_model is None:
            return "Object detection model not loaded.", None, None, []
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            return "Failed to open video.", None, None, []
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        os.makedirs('saved_files', exist_ok=True)
        saved_video_path = f"saved_files/processed_video_{int(time.time())}.mp4"
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(saved_video_path, fourcc, fps, (width, height))
        description = ""
        frame_descriptions = []
        all_objects = []
        previous_objects = set()
        previous_boxes = []
        step = 5
        for i in range(0, frame_count, step):
            cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            ret, frame = cap.read()
            if not ret:
                break
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            image = Image.fromarray(frame_rgb).resize((640, 480))
            results = yolo_model.predict(image, imgsz=640, conf=0.6)
            boxes = results[0].boxes.xyxy.cpu().numpy()
            confidences = results[0].boxes.conf.cpu().numpy()
            class_names = [results[0].names[int(cls)] for cls in results[0].boxes.cls]
            detected_objects = set(name for name, conf in zip(class_names, confidences) if conf > 0.6)
            object_counts = Counter(detected_objects)
            if midas and midas_transform:
                img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                input_batch = midas_transform(img).to(device)
                with torch.no_grad():
                    depth_map = midas(input_batch).cpu().numpy().squeeze()
                depth_map = cv2.resize(depth_map, (width, height))
            else:
                depth_map = None
            if detected_objects != previous_objects:
                scale_x = width / 640
                scale_y = height / 480
                objects = []
                current_boxes = []
                for box, cls, conf in zip(boxes, results[0].boxes.cls, confidences):
                    if conf > 0.6:
                        x1, y1, x2, y2 = map(int, box)
                        x1, x2 = int(x1 * scale_x), int(x2 * scale_x)
                        y1, y2 = int(y1 * scale_y), int(y2 * scale_y)
                        class_name = results[0].names[int(cls)]
                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)
                        cv2.putText(frame, f"{class_name} ({conf:.2f})", (x1, y1 - 10),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
                        dist = estimate_distance(depth_map, [x1, y1, x2, y2], focal_length=600) if depth_map is not None else "Distance not estimated"
                        objects.append({
                            'name': class_name,
                            'x1': float(x1),
                            'y1': float(y1),
                            'x2': float(x2),
                            'y2': float(y2),
                            'new': True,
                            'distance': dist
                        })
                        current_boxes.append({'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2, 'class_name': class_name, 'conf': conf})
                all_objects.extend(objects)
                previous_boxes = current_boxes
                frame_description = f"Frame {i+1}: "
                if detected_objects:
                    frame_description += f"Detected {len(detected_objects)} unique objects: " if language == 'en' else f"{len(detected_objects)} شیء منحصر به فرد شناسایی شد: "
                    frame_description += ', '.join([f"{count} {obj}" if count > 1 else obj for obj, count in object_counts.items()]) + ". "
                for obj in objects:
                    frame_description += f"{obj['name']} is {obj['distance']}. " if language == 'en' else f"{obj['name']} در {obj['distance']} قرار دارد. "
                if blip_processor and blip_model:
                    inputs = blip_processor(images=image, return_tensors="pt").to(device)
                    caption = blip_model.generate(**inputs, max_length=100, num_beams=5, no_repeat_ngram_size=3)
                    caption_text = blip_processor.decode(caption[0], skip_special_tokens=True)
                    if len(caption_text.split()) < 3:
                        caption_text = "A lively scene with various objects." if language == 'en' else "صحنه‌ای پرجنب‌وجوش با اشیای مختلف."
                else:
                    caption_text = "Fallback: Scene with objects."
                if gemma_model and gemma_tokenizer:
                    prompt = f"Refine this caption for {user_type} users in {language} to enhance accessibility: Describe key elements, spatial layout, estimated distances, and object interactions clearly and concisely, without repetition or unnecessary details: {caption_text}"
                    inputs = gemma_tokenizer(prompt, return_tensors="pt").to(device)
                    output = gemma_model.generate(**inputs, max_new_tokens=100, temperature=0.6, do_sample=True, no_repeat_ngram_size=3)
                    caption_text = gemma_tokenizer.decode(output[0][len(inputs.input_ids[0]):], skip_special_tokens=True)
                    frame_description += f"Description: {caption_text}. "
                frame_descriptions.append(frame_description)
                previous_objects = detected_objects
            else:
                for prev_box in previous_boxes:
                    x1, y1, x2, y2 = prev_box['x1'], prev_box['y1'], prev_box['x2'], prev_box['y2']
                    class_name, conf = prev_box['class_name'], prev_box['conf']
                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)
                    cv2.putText(frame, f"{class_name} ({conf:.2f})", (x1, y1 - 10),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
            out.write(frame)
        cap.release()
        out.release()
        base64_file = b64encode(open(saved_video_path, 'rb').read()).decode('utf-8')
        description = "Video processed." if language == 'en' else "ویدیو پردازش شد. "
        description += " ".join(frame_descriptions)
        if user_type == 'deaf':
            subtitles = generate_subtitles(video_path, language)
            description += f"\nSubtitles: {subtitles} 📢"
            description += " (Sign language translation not implemented) 🤟"
            description += " 🚨 Alert: Objects detected! 📸"
        return description, saved_video_path, base64_file, all_objects
    except Exception as e:
        print(f"Video processing error: {str(e)}")
        return f"Video processing error: {str(e)}", None, None, []

html_content = """
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="MD: VisionAid+DeafAid - An AI-powered assistive tool for blind and deaf users with video and object detection">
    <meta name="theme-color" content="#1d4ed8">
    <title>MD: VisionAid+DeafAid</title>
    <link rel="manifest" href="/manifest.json">
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;700&family=Roboto:wght@400;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', 'Roboto', sans-serif;
            background-color: #f8fafc;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            padding: 1.5rem;
            transition: all 0.3s ease;
        }
        body.blind-mode {
            background-color: #121212;
            color: #e6f3ff;
            font-size: 2.5rem;
            line-height: 3rem;
        }
        body.deaf-mode {
            background: linear-gradient(135deg, #60a5fa, #a5b4fc);
            color: #1e293b;
            font-size: 1.125rem;
        }
        .container {
            background: #ffffff;
            border-radius: 1.5rem;
            padding: 2rem;
            box-shadow: 0 10px 20px rgba(0, 0, 0, 0.1);
            max-width: 48rem;
            width: 100%;
            transition: all 0.3s ease;
        }
        .output-container {
            border-radius: 1rem;
            padding: 1.5rem;
            background-color: #f1f5f9;
            transition: all 0.3s ease;
        }
        .output-container.blind-mode {
            background-color: #1f2937;
            border: 2px solid #60a5fa;
            box-shadow: 0 6px 12px rgba(255, 255, 255, 0.1);
            font-size: 2rem;
        }
        .output-container.deaf-mode {
            background-color: #e0f2fe;
            border: 3px solid #2563eb;
            box-shadow: 0 8px 16px rgba(0, 0, 0, 0.15);
        }
        button {
            transition: all 0.3s ease;
            border-radius: 0.75rem;
            padding: 0.75rem 1.5rem;
            font-weight: 600;
            letter-spacing: 0.025rem;
        }
        button.blind-mode {
            background-color: #2563eb;
            color: #ffffff;
            border: 2px solid #60a5fa;
        }
        button.blind-mode:hover {
            background-color: #1e40af;
            transform: translateY(-2px);
            box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2);
        }
        button.deaf-mode {
            background-color: #3b82f6;
            color: #ffffff;
            border: 2px solid #93c5fd;
        }
        button.deaf-mode:hover {
            background-color: #2563eb;
            transform: translateY(-2px);
            box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2);
        }
        select, input {
            transition: all 0.3s ease;
            border-radius: 0.75rem;
            padding: 0.75rem 1rem;
            border: 2px solid #d1d5db;
            background-color: #f9fafb;
            font-size: 1rem;
        }
        select:focus, input:focus {
            border-color: #2563eb;
            outline: none;
            box-shadow: 0 0 0 4px rgba(37, 99, 235, 0.2);
        }
        [aria-label]:focus::after {
            content: attr(aria-label);
            position: absolute;
            top: -2.5rem;
            background: #1e293b;
            color: #fff;
            padding: 0.5rem 1rem;
            border-radius: 0.5rem;
            font-size: 0.875rem;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
        }
        @media (max-width: 640px) {
            body.blind-mode {
                font-size: 1.5rem;
                line-height: 2rem;
            }
            .container {
                padding: 1.25rem;
            }
            .output-container {
                padding: 1rem;
            }
            button, select, input {
                padding: 0.5rem 0.75rem;
                font-size: 0.875rem;
            }
            h1 {
                font-size: 2rem;
            }
            p {
                font-size: 1rem;
            }
        }
        h1 {
            font-size: 2.5rem;
            font-weight: 800;
            color: #1e3a8a;
            text-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            animation: fadeIn 1s ease-in;
        }
        h3 {
            font-size: 1.25rem;
            font-weight: 700;
            color: #1e293b;
        }
        p {
            font-size: 1.125rem;
            color: #4b5563;
        }
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(-10px); }
            to { opacity: 1; transform: translateY(0); }
        }
        img, video {
            max-width: 100%;
            border-radius: 1rem;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            transition: transform 0.3s ease;
        }
        img:hover, video:hover {
            transform: scale(1.02);
        }
        .visual-alert {
            animation: flash 1s infinite;
        }
        @keyframes flash {
            0% { opacity: 1; }
            50% { opacity: 0; }
            100% { opacity: 1; }
        }
    </style>
</head>
<body class="deaf-mode min-h-screen flex flex-col items-center justify-content p-6">
    <h1 class="text-4xl font-extrabold mb-6 text-blue-800 animate-pulse" aria-label="MD: VisionAid+DeafAid Title">MD: VisionAid+DeafAid</h1>
    <p class="text-xl mb-8 text-gray-900 max-w-2xl text-center" aria-label="Description">An AI-powered assistive tool for blind and deaf users with video and object detection</p>
    <div class="container">
        <div class="mb-6">
            <label for="user_type" class="block text-lg font-semibold text-gray-900 mb-2" aria-label="User Type Label">User Type:</label>
            <select id="user_type" class="block w-full p-3 border-2 border-gray-300 rounded-lg bg-gray-50 focus:border-blue-600" onchange="updateUserMode()" aria-label="Select user type">
                <option value="blind">Blind</option>
                <option value="deaf">Deaf</option>
            </select>
        </div>
        <div class="mb-6">
            <label for="language" class="block text-lg font-semibold text-gray-900 mb-2" aria-label="Language Label">Language:</label>
            <select id="language" class="block w-full p-3 border-2 border-gray-300 rounded-lg bg-gray-50 focus:border-blue-600" aria-label="Select language">
                <option value="en">English</option>
                <option value="fa">فارسی</option>
            </select>
        </div>
        <div class="mb-6">
            <h3 class="text-xl font-semibold text-gray-900 mb-3" aria-label="Target Coordinates Section">Target Coordinates</h3>
            <input type="number" step="any" id="object_lat" placeholder="Target Latitude (e.g., 35.1357)" class="block w-full p-3 border-2 border-gray-300 rounded-lg bg-gray-50 focus:border-blue-600 mb-3" aria-label="Enter target latitude">
            <input type="number" step="any" id="object_lon" placeholder="Target Longitude (e.g., -121.8269)" class="block w-full p-3 border-2 border-gray-300 rounded-lg bg-gray-50 focus:border-blue-600" aria-label="Enter target longitude">
        </div>
        <div class="mb-6">
            <h3 class="text-xl font-semibold text-gray-900 mb-3" aria-label="User Coordinates Section">User Coordinates (Optional)</h3>
            <input type="number" step="any" id="user_lat" placeholder="Your Latitude (Optional)" class="block w-full p-3 border-2 border-gray-300 rounded-lg bg-gray-50 focus:border-blue-600 mb-3" aria-label="Enter your latitude">
            <input type="number" step="any" id="user_lon" placeholder="Your Longitude (Optional)" class="block w-full p-3 border-2 border-gray-300 rounded-lg bg-gray-50 focus:border-blue-600" aria-label="Enter your longitude">
        </div>
        <div class="mb-6">
            <h3 class="text-xl font-semibold text-gray-900 mb-3" aria-label="Input Section">Image or Video Input</h3>
            <input type="file" id="media_input" accept="image/*,video/*" class="block w-full p-2 border-2 border-gray-300 rounded-lg bg-gray-50" aria-label="Upload image or video">
        </div>
        <div class="mb-6">
            <h3 class="text-xl font-semibold text-gray-900 mb-3" aria-label="Audio Input Section">Audio Input</h3>
            <button id="listen-btn" onclick="startListening()" class="w-full bg-purple-600 text-white px-6 py-3 rounded-lg hover:bg-purple-700" aria-label="Start listening">Start Listening</button>
            <div id="recording-status" class="text-red-600 mt-3 text-center font-medium" aria-live="polite"></div>
        </div>
        <div class="mb-6">
            <h3 class="text-xl font-semibold text-gray-900 mb-3" aria-label="Query Section">Query</h3>
            <input type="text" id="query" placeholder="Ask MD (e.g., MD, process video or where am I)" class="block w-full p-3 border-2 border-gray-300 rounded-lg bg-gray-50 focus:border-blue-600" aria-label="Enter query">
            <button id="process-btn" onclick="processInput()" class="w-full mt-3 bg-blue-600 text-white px-6 py-3 rounded-lg hover:bg-blue-700" aria-label="Process input">Process</button>
        </div>
        <div class="mb-6">
            <h3 class="text-xl font-semibold text-gray-900 mb-3" aria-label="Output Section">Output</h3>
            <div id="output" class="output-container p-6 rounded-lg bg-gray-50" aria-live="polite"></div>
            <img id="processed_image" src="" style="display:none;" class="mt-4" aria-label="Processed image with detected objects">
            <video id="processed_video" controls style="display:none;" class="mt-4" aria-label="Processed video with detected objects"></video>
            <audio id="audio_output" controls class="mt-4 w-full" aria-label="Play audio output"></audio>
            <textarea id="history-log" style="display:none;"></textarea>
        </div>
    </div>
    <script>
        let listening = false;
        const recordingStatus = document.getElementById('recording-status');
        const output = document.getElementById('output');
        const audioOutput = document.getElementById('audio_output');
        const queryInput = document.getElementById('query');
        const processedImage = document.getElementById('processed_image');
        const processedVideo = document.getElementById('processed_video');
        console.log('JavaScript loaded successfully');
        const translations = {
            en: {
                title: "MD: VisionAid+DeafAid",
                description: "An AI-powered assistive tool for blind and deaf users with video and object detection",
                userTypeLabel: "User Type:",
                userTypeBlind: "Blind",
                userTypeDeaf: "Deaf",
                languageLabel: "Language:",
                targetCoords: "Target Coordinates",
                targetLat: "Target Latitude (e.g., 35.1357)",
                targetLon: "Target Longitude (e.g., -121.8269)",
                userCoords: "Your Coordinates (Optional)",
                userLat: "Your Latitude (Optional)",
                userLon: "Your Longitude (Optional)",
                mediaInput: "Image or Video Input",
                audioInput: "Audio Input",
                startListening: "Start Listening",
                querySection: "Query",
                queryPlaceholder: "Ask MD (e.g., MD, process video or where am I)",
                process: "Process",
                outputSection: "Output"
            },
            fa: {
                title: "MD: VisionAid+DeafAid",
                description: "یک ابزار کمکی مبتنی بر هوش مصنوعی برای کاربران نابینا و ناشنوا با تشخیص ویدیو و شیء",
                userTypeLabel: "نوع کاربر:",
                userTypeBlind: "نابینا",
                userTypeDeaf: "ناشنوا",
                languageLabel: "زبان:",
                targetCoords: "مختصات هدف",
                targetLat: "عرض جغرافیایی هدف (مثال: 35.1357)",
                targetLon: "طول جغرافیایی هدف (مثال: -121.8269)",
                userCoords: "مختصات شما (اختیاری)",
                userLat: "عرض جغرافیایی شما (اختیاری)",
                userLon: "طول جغرافیایی شما (اختیاری)",
                mediaInput: "ورودی تصویر یا ویدیو",
                audioInput: "ورودی صوتی",
                startListening: "شروع گوش دادن",
                querySection: "پرس‌وجو",
                queryPlaceholder: "از MD بپرسید (مثال: MD، ویدیو را پردازش کن یا من کجا هستم)",
                process: "پردازش",
                outputSection: "خروجی"
            }
        };
        function updateLanguage() {
            const lang = document.getElementById('language').value;
            document.querySelector('h1').innerText = translations[lang].title;
            document.querySelector('p.text-xl').innerText = translations[lang].description;
            document.querySelector('label[for="user_type"]').innerText = translations[lang].userTypeLabel;
            document.querySelector('option[value="blind"]').innerText = translations[lang].userTypeBlind;
            document.querySelector('option[value="deaf"]').innerText = translations[lang].userTypeDeaf;
            document.querySelector('label[for="language"]').innerText = translations[lang].languageLabel;
            document.querySelector('h3[aria-label="Target Coordinates Section"]').innerText = translations[lang].targetCoords;
            document.getElementById('object_lat').placeholder = translations[lang].targetLat;
            document.getElementById('object_lon').placeholder = translations[lang].targetLon;
            document.querySelector('h3[aria-label="User Coordinates Section"]').innerText = translations[lang].userCoords;
            document.getElementById('user_lat').placeholder = translations[lang].userLat;
            document.getElementById('user_lon').placeholder = translations[lang].userLon;
            document.querySelector('h3[aria-label="Input Section"]').innerText = translations[lang].mediaInput;
            document.querySelector('h3[aria-label="Audio Input Section"]').innerText = translations[lang].audioInput;
            document.getElementById('listen-btn').innerText = translations[lang].startListening;
            document.querySelector('h3[aria-label="Query Section"]').innerText = translations[lang].querySection;
            document.getElementById('query').placeholder = translations[lang].queryPlaceholder;
            document.getElementById('process-btn').innerText = translations[lang].process;
            document.querySelector('h3[aria-label="Output Section"]').innerText = translations[lang].outputSection;
        }
        document.getElementById('language').addEventListener('change', updateLanguage);
        updateLanguage();
        function getUserLocation() {
            try {
                if (navigator.geolocation) {
                    navigator.geolocation.getCurrentPosition((position) => {
                        document.getElementById('user_lat').value = position.coords.latitude;
                        document.getElementById('user_lon').value = position.coords.longitude;
                        output.innerText = document.getElementById('language').value === 'fa' ? 'مختصات GPS دریافت شد.' : 'GPS coordinates acquired.';
                        if (document.getElementById('user_type').value === 'blind') speakOnServer(output.innerText);
                    }, (error) => {
                        output.innerText = document.getElementById('language').value === 'fa' ? 'دسترسی به GPS رد شد. لطفاً مختصات را دستی وارد کنید.' : 'GPS access denied. Please enter coordinates manually.';
                        if (document.getElementById('user_type').value === 'blind') speakOnServer(output.innerText);
                        console.error('GPS error: ' + error.message);
                    });
                } else {
                    output.innerText = document.getElementById('language').value === 'fa' ? 'مرورگر از موقعیت‌یابی پشتیبانی نمی‌کند.' : 'Geolocation not supported by browser.';
                }
            } catch (e) {
                output.innerText = document.getElementById('language').value === 'fa' ? `خطای موقعیت‌یابی: ${e.message}` : `Location error: ${e.message}`;
            }
        }
        getUserLocation();
        function updateButtonStyles() {
            console.log('updateButtonStyles called');
            const userType = document.getElementById('user_type').value;
            const buttons = document.querySelectorAll('button');
            buttons.forEach(btn => {
                btn.classList.toggle('blind-mode', userType === 'blind');
                btn.classList.toggle('deaf-mode', userType === 'deaf');
            });
        }
        async function speakOnServer(message) {
            try {
                const response = await fetch('/generate_audio', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        message: message,
                        language: document.getElementById('language').value
                    })
                });
                const result = await response.json();
                if (result.audio_url && document.getElementById('user_type').value === 'blind') {
                    audioOutput.src = result.audio_url;
                    audioOutput.play();
                    navigator.vibrate([200]);
                }
            } catch (e) {
                console.error('Server speech error:', e);
                output.innerText = document.getElementById('language').value === 'fa' ? `خطای تولید صوت: ${e}` : `Speech generation error: ${e}`;
                queryInput.value = output.innerText;
            }
        }
        async function checkMicPermission() {
            try {
                const permission = await navigator.permissions.query({ name: 'microphone' });
                if (permission.state === 'denied') {
                    output.innerText = document.getElementById('language').value === 'fa' ? 'دسترسی به میکروفون رد شد. لطفاً آن را در تنظیمات مرورگر فعال کنید.' : 'Microphone access denied. Please enable it in browser settings.';
                    queryInput.value = output.innerText;
                    await speakOnServer(output.innerText);
                    return false;
                }
                return true;
            } catch (e) {
                output.innerText = document.getElementById('language').value === 'fa' ? `خطای بررسی مجوز میکروفون: ${e}` : `Microphone permission check error: ${e}`;
                queryInput.value = output.innerText;
                await speakOnServer(output.innerText);
                return false;
            }
        }
        async function updateUserMode() {
            try {
                const userType = document.getElementById('user_type').value;
                document.body.className = userType === 'blind' ? 'blind-mode' : 'deaf-mode';
                output.className = `output-container p-6 rounded-lg ${userType === 'blind' ? 'blind-mode' : 'deaf-mode bg-gray-50'}`;
                updateButtonStyles();
                const message = userType === 'blind' ?
                    (document.getElementById('language').value === 'fa' ? 'حالت کاربر به نابینا تنظیم شد. برای فعال‌سازی بگویید "MD".' : 'User mode set to blind. Say "MD" to activate.') :
                    (document.getElementById('language').value === 'fa' ? 'حالت کاربر به ناشنوا تنظیم شد. خروجی‌ها به‌صورت بصری نمایش داده می‌شوند.' : 'User mode set to deaf. Outputs are displayed visually.');
                output.innerText = message;
                queryInput.value = message;
                if (userType === 'blind') {
                    audioOutput.style.display = 'block';
                    output.style.fontSize = '2rem';
                    processedImage.style.display = 'none';
                    processedVideo.style.display = 'none';
                    await speakOnServer(message);
                    navigator.vibrate([200, 100, 200]);
                } else {
                    audioOutput.style.display = 'none';
                    output.style.fontSize = '1.125rem';
                    processedImage.style.display = 'block';
                    processedVideo.style.display = 'block';
                    output.classList.add('visual-alert');
                }
            } catch (e) {
                output.innerText = document.getElementById('language').value === 'fa' ? `خطای تغییر حالت کاربر: ${e}` : `User mode change error: ${e}`;
                queryInput.value = output.innerText;
                await speakOnServer(output.innerText);
            }
        }
        async function startListening() {
            try {
                if (!(await checkMicPermission())) return;
                const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
                recognition.lang = document.getElementById('language').value === 'fa' ? 'fa-IR' : 'en-US';
                recognition.onstart = () => {
                    listening = true;
                    recordingStatus.classList.add('recording');
                    recordingStatus.innerText = document.getElementById('language').value === 'fa' ? 'در حال گوش دادن...' : 'Listening...';
                };
                recognition.onresult = async (event) => {
                    const transcript = event.results[0][0].transcript;
                    queryInput.value = transcript;
                    listening = false;
                    recordingStatus.classList.remove('recording');
                    recordingStatus.innerText = '';
                    if (transcript.toLowerCase().includes('process video') || transcript.toLowerCase().includes('پردازش ویدیو') ||
                        transcript.toLowerCase().includes('where am i') || transcript.toLowerCase().includes('من کجا هستم')) {
                        await processInput();
                    }
                    if (document.getElementById('user_type').value === 'blind') {
                        await speakOnServer(document.getElementById('language').value === 'fa' ? 'متن صوتی در فیلد ورودی قرار گرفت.' : 'Voice input placed in query field.');
                        navigator.vibrate([200]);
                    } else if (document.getElementById('user_type').value === 'deaf') {
                        recordingStatus.innerText = '🗣️ Input received!';
                        recordingStatus.classList.add('visual-alert');
                        setTimeout(() => recordingStatus.classList.remove('visual-alert'), 2000);
                    }
                };
                recognition.onend = () => {
                    listening = false;
                    recordingStatus.classList.remove('recording');
                    recordingStatus.innerText = '';
                };
                recognition.onerror = async (event) => {
                    output.innerText = document.getElementById('language').value === 'fa' ? `خطای تشخیص صوت: ${event.error}` : `Speech recognition error: ${event.error}`;
                    queryInput.value = output.innerText;
                    if (document.getElementById('user_type').value === 'blind') await speakOnServer(output.innerText);
                };
                recognition.start();
            } catch (e) {
                output.innerText = document.getElementById('language').value === 'fa' ? `خطای شروع گوش دادن: ${e}` : `Listening error: ${e}`;
                queryInput.value = output.innerText;
                if (document.getElementById('user_type').value === 'blind') await speakOnServer(output.innerText);
            }
        }
        async function processInput(formData = new FormData()) {
            try {
                formData.append('user_type', document.getElementById('user_type').value);
                formData.append('language', document.getElementById('language').value);
                formData.append('query', document.getElementById('query').value);
                formData.append('object_lat', document.getElementById('object_lat').value || 35.1357);
                formData.append('object_lon', document.getElementById('object_lon').value || -121.8269);
                formData.append('user_lat', document.getElementById('user_lat').value || '');
                formData.append('user_lon', document.getElementById('user_lon').value || '');
                if (document.getElementById('media_input').files[0]) {
                    formData.append('media', document.getElementById('media_input').files[0]);
                }
                output.innerText = document.getElementById('language').value === 'fa' ? 'در حال پردازش...' : 'Processing...';
                queryInput.value = output.innerText;
                const response = await fetch('/process_input', {
                    method: 'POST',
                    body: formData
                });
                const result = await response.json();
                console.log('Fetch response:', result);
                if (result.error) {
                    output.innerText = result.error;
                    queryInput.value = result.error;
                    processedImage.style.display = 'none';
                    processedVideo.style.display = 'none';
                    if (document.getElementById('user_type').value === 'blind') {
                        await speakOnServer(result.error);
                        navigator.vibrate([500]);
                    } else if (document.getElementById('user_type').value === 'deaf') {
                        output.classList.add('visual-alert');
                        setTimeout(() => output.classList.remove('visual-alert'), 2000);
                    }
                } else {
                    output.innerHTML = result.visual_output;
                    queryInput.value = result.description;
                    document.getElementById('history-log').value += `\n${result.description}`;
                    if (result.audio_url && document.getElementById('user_type').value === 'blind') {
                        audioOutput.src = result.audio_url;
                        audioOutput.play();
                    }
                    if (result.saved_file_path && document.getElementById('user_type').value === 'deaf') {
                        output.innerHTML += `<br><a href="${result.saved_file_url}" download>دانلود فایل پردازش‌شده</a>`;
                        if (result.saved_file_path.endsWith('.mp4')) {
                            processedVideo.src = `data:video/mp4;base64,${result.file_base64}`;
                            processedVideo.style.display = 'block';
                            processedImage.style.display = 'none';
                        } else {
                            processedImage.src = `data:image/jpeg;base64,${result.file_base64}`;
                            processedImage.style.display = 'block';
                            processedVideo.style.display = 'none';
                        }
                    } else {
                        processedImage.style.display = 'none';
                        processedVideo.style.display = 'none';
                    }
                }
            } catch (e) {
                output.innerText = document.getElementById('language').value === 'fa' ? `خطا: ${e}` : `Error: ${e}`;
                queryInput.value = output.innerText;
                processedImage.style.display = 'none';
                processedVideo.style.display = 'none';
                if (document.getElementById('user_type').value === 'blind') {
                    await speakOnServer(output.innerText);
                    navigator.vibrate([500]);
                } else if (document.getElementById('user_type').value === 'deaf') {
                    output.classList.add('visual-alert');
                    setTimeout(() => output.classList.remove('visual-alert'), 2000);
                }
                console.error('Fetch error:', e);
            }
        }
        document.addEventListener('keydown', async (e) => {
            try {
                if (e.key === 'Enter' && document.activeElement.id === 'query') await processInput();
                if (e.key === 'l' && !listening) await startListening();
            } catch (e) {
                output.innerText = document.getElementById('language').value === 'fa' ? `خطای کلید: ${e}` : `Key error: ${e}`;
                queryInput.value = output.innerText;
                if (document.getElementById('user_type').value === 'blind') await speakOnServer(output.innerText);
            }
        });
        if ('serviceWorker' in navigator) {
            navigator.serviceWorker.register('/service-worker.js').then(() => {
                console.log('Service Worker registered');
            }).catch(e => console.error('Service Worker registration failed:', e));
        }
        updateUserMode();
    </script>
</body>
</html>
"""

service_worker_content = """
self.addEventListener('install', event => {
    console.log('Service Worker installing');
    event.waitUntil(
        caches.open('md-visionaid-cache-v1').then(cache => {
            return cache.addAll([
                '/',
                '/manifest.json'
            ]);
        })
    );
});
self.addEventListener('fetch', event => {
    console.log('Service Worker fetching:', event.request.url);
    event.respondWith(
        caches.match(event.request).then(response => {
            return response || fetch(event.request);
        })
    );
});
"""

manifest_content = """
{
    "name": "MD: VisionAid+DeafAid",
    "short_name": "MD",
    "description": "An AI-powered assistive tool for blind and deaf users with video and object detection",
    "start_url": "/",
    "display": "standalone",
    "background_color": "#020202",
    "theme_color": "#1d4ed8",
    "icons": []
}
"""

def calculate_distance(lat1, lon1, lat2, lon2):
    try:
        R = 6371008
        lat1_rad = math.radians(float(lat1))
        lon1_rad = math.radians(float(lon1))
        lat2_rad = math.radians(float(lat2))
        lon2_rad = math.radians(float(lon2))
        dlat = lat2_rad - lat1_rad
        dlon = lon2_rad - lon1_rad
        a = math.sin(dlat / 2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon / 2)**2
        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
        distance = R * c
        bearing = math.atan2(math.sin(dlon)*math.cos(lat2_rad), math.cos(lat1_rad)*math.sin(lat2_rad)-math.sin(lat1_rad)*math.cos(lat2_rad)*math.cos(dlon))
        bearing = math.degrees(bearing)
        bearing = (bearing + 360) % 360
        directions = ['North', 'Northeast', 'East', 'Southeast', 'South', 'Southwest', 'West', 'Northwest']
        dir_idx = round(bearing / 45) % 8
        direction = directions[dir_idx]
        return distance, direction
    except Exception as e:
        print(f"Distance calculation error: {str(e)}")
        return None, None

def analyze_input(media, audio_path, language='en', query=None, user_type='blind', user_lat=None, user_lon=None):
    description = ""
    saved_file_path = None
    base64_file = None
    objects = []
    commands = {
        'en': {
            'save image': 'Image saved for later use.',
            'save video': 'Video frame saved for later use.',
            'process video': 'Processing video.',
            'hello': 'Hello! How can MD assist you today?',
            'where am i': 'Describing your current location.',
            'describe scene': 'Describing the current scene.',
            'find object': 'Searching for specified object.',
            'navigate to': 'Providing navigation directions.',
            'help': 'Listing available commands.',
            'read text': 'Reading text from image.',
            'detect colors': 'Detecting dominant colors.',
            'generate subtitles': 'Generating subtitles for video.',
            'switch language': 'Switching language.',
            'enable alerts': 'Enabling visual alerts.',
            'process image': 'Processing image.',
            'get distance': 'Estimating distance to objects.',
            'detect objects': 'Detecting objects in media.',
            'get coordinates': 'Retrieving current GPS coordinates.',
            'clean files': 'Cleaning old files.'
        },
        'fa': {
            'ذخیره تصویر': 'تصویر برای استفاده بعدی ذخیره شد.',
            'ذخیره ویدیو': 'فریم ویدیویی برای استفاده بعدی ذخیره شد.',
            'پردازش ویدیو': 'پردازش ویدیو.',
            'سلام': 'سلام! MD چگونه می‌تواند به شما کمک کند؟',
            'من کجا هستم': 'توصیف مکان فعلی شما.',
            'توصیف صحنه': 'توصیف صحنه فعلی.',
            'پیدا کردن شی': 'جستجوی شی مشخص‌شده.',
            'ناوبری به': 'ارائه جهت‌های ناوبری.',
            'کمک': 'لیست دستورات موجود.',
            'خواندن متن': 'خواندن متن از تصویر.',
            'تشخیص رنگ‌ها': 'تشخیص رنگ‌های غالب.',
            'تولید زیرنویس': 'تولید زیرنویس برای ویدیو.',
            'تغییر زبان': 'تغییر زبان.',
            'فعال کردن هشدارها': 'فعال کردن هشدارهای بصری.',
            'پردازش تصویر': 'پردازش تصویر.',
            'دریافت فاصله': 'تخمین فاصله تا اشیا.',
            'تشخیص اشیا': 'تشخیص اشیا در رسانه.',
            'دریافت مختصات': 'دریافت مختصات GPS فعلی.',
            'پاک کردن فایل‌ها': 'پاک کردن فایل‌های قدیمی.'
        }
    }
    try:
        if query and gemma_model and gemma_tokenizer:
            with torch.no_grad():
                prompt = f"Interpret this user query for an assistive AI in {language}, extract main command (e.g., 'process video'), and suggest response structure: {query}"
                inputs = gemma_tokenizer(prompt, return_tensors="pt").to(device)
                output = gemma_model.generate(**inputs, max_new_tokens=50, temperature=0.7, do_sample=True)
                command = gemma_tokenizer.decode(output[0][len(inputs.input_ids[0]):], skip_special_tokens=True).lower()
        else:
            query = query.lower() if query else ""
            command = query
        latitude = float(user_lat) if user_lat and user_lat != '' else None
        longitude = float(user_lon) if user_lon and user_lon != '' else None
        if latitude is None or longitude is None:
            try:
                g = geocoder.ip('me')
                if g.ok:
                    latitude, longitude = g.latlng
                else:
                    latitude, longitude = 35.6895, 139.6917
                    description += "Unable to retrieve GPS coordinates. Using default location. " if language == 'en' else "نمی‌توان مختصات GPS را دریافت کرد. از مکان پیش‌فرض استفاده می‌شود. "
            except Exception as e:
                description += f"GPS retrieval error: {str(e)}. " if language == 'en' else f"خطای دریافت GPS: {str(e)}. "
                print(f"Geocoder error: {str(e)}")
        if command in commands[language]:
            if gemma_model and gemma_tokenizer:
                with torch.no_grad():
                    prompt = f"Generate a natural response in {language} for this command in an assistive app: {command}"
                    inputs = gemma_tokenizer(prompt, return_tensors="pt").to(device)
                    output = gemma_model.generate(**inputs, max_new_tokens=50, temperature=0.7, do_sample=True)
                    description += gemma_tokenizer.decode(output[0][len(inputs.input_ids[0]):], skip_special_tokens=True) + " "
            else:
                description += commands[language][command] + " "
        if media:
            clean_old_files()
            media_type = media.content_type if hasattr(media, 'content_type') else 'image'
            if 'video' in media_type:
                os.makedirs('saved_files', exist_ok=True)
                video_path = f"saved_files/video_{int(time.time())}.mp4"
                media.save(video_path)
                video_description, saved_file_path, base64_file, objects = process_video(video_path, language, user_type)
                description += video_description
                if command in ['save video', 'ذخیره ویدیو']:
                    description += f"Video saved at {saved_file_path}. " if language == 'en' else f"ویدیو در {saved_file_path} ذخیره شد. "
                if os.path.exists(video_path):
                    os.remove(video_path)
            else:
                image = Image.open(media.stream if hasattr(media, 'stream') else BytesIO(media.read())).convert('RGB').resize((640, 480))
                image_description, saved_file_path, base64_file, objects = process_image_with_objects(image, language, user_type, query)
                description += image_description
                if command in ['save image', 'ذخیره تصویر']:
                    description += f"Image saved at {saved_file_path}. " if language == 'en' else f"تصویر در {saved_file_path} ذخیره شد. "
        if query and ("where am i" in command or (language == 'fa' and "من کجا هستم" in command)):
            try:
                if latitude is None or longitude is None:
                    description += "GPS coordinates not provided. Please ensure location services are enabled or enter manually. " if language == 'en' else "مختصات GPS ارائه نشده است. لطفاً خدمات موقعیت‌یابی را فعال کنید یا دستی وارد کنید. "
                else:
                    location_description = (
                        f"Your approximate coordinates are Latitude {latitude:.4f}, Longitude {longitude:.4f}. "
                        if language == 'en' else
                        f"مختصات تقریبی شما عرض جغرافیایی {latitude:.4f}، طول جغرافیایی {longitude:.4f} است."
                    )
                    if objects:
                        location_description += f"Detected {len(objects)} objects: " if language == 'en' else f"{len(objects)} شیء شناسایی شد: "
                        object_counts = Counter([obj['name'] for obj in objects])
                        location_description += ', '.join([f"{count} {obj}" if count > 1 else obj for obj, count in object_counts.items()]) + ". "
                    description += location_description
                    try:
                        object_lat = float(request.form.get('object_lat', latitude or 35.1357))
                        object_lon = float(request.form.get('object_lon', longitude or -121.8269))
                        distance, direction = calculate_distance(latitude or 35.6895, longitude or 139.6917, object_lat, object_lon)
                        steps = int(distance / 0.75) if distance is not None else 0
                        if distance is not None:
                            gps_data = (
                                f"You are approximately {steps} steps away from the target at Latitude {object_lat:.4f}, Longitude {object_lon:.4f}, heading {direction}. "
                                if language == 'en' else
                                f"شما تقریباً {steps} قدم از هدف در عرض جغرافیایی {object_lat:.4f}، طول جغرافیایی {object_lon:.4f} به سمت {direction} فاصله دارید."
                            )
                            description += gps_data
                        else:
                            description += "Unable to calculate distance to target. " if language == 'en' else "نمی‌توان فاصله تا هدف را محاسبه کرد. "
                    except Exception as e:
                        description += f"GPS calculation error: {str(e)}. " if language == 'en' else f"خطای محاسبه GPS: {str(e)}. "
                        print(f"GPS calculation error: {str(e)}")
            except Exception as e:
                description += f"Location processing error: {str(e)}. " if language == 'en' else f"خطای پردازش مکان: {str(e)}. "
                print(f"Location processing error: {str(e)}")
        if query and gemma_model and gemma_tokenizer:
            try:
                target_query = query.strip().lower()
                if target_query.startswith("md"):
                    target_query = target_query[2:].strip()
                if "greate" in target_query:
                    target_query = target_query.replace("greate", "great")
                if not any(key in target_query for key in commands[language]):
                    prompt = (
                        f"Provide a clear and concise response in one sentence for {'blind users' if user_type == 'blind' else 'deaf users'} in {language}, interpreting the query as a request for information about the environment or objects, avoiding repetition: {target_query}"
                        if language == 'en' else
                        f"یک پاسخ واضح و مختصر در یک جمله برای {'کاربران نابینا' if user_type == 'blind' else 'کاربران ناشنوا'} به {language} ارائه دهید، با تفسیر پرس‌وجو به‌عنوان درخواست اطلاعات درباره محیط یا اشیا، بدون تکرار: {target_query}"
                    )
                    inputs = gemma_tokenizer(prompt, return_tensors="pt").to(device)
                    output = gemma_model.generate(**inputs, max_new_tokens=50, temperature=0.7, do_sample=True, no_repeat_ngram_size=3)
                    query_response = gemma_tokenizer.decode(output[0][len(inputs.input_ids[0]):], skip_special_tokens=True)
                    description += f"Response: {query_response}. " if language == 'en' else f"پاسخ: {query_response}. "
            except Exception as e:
                description += f"Query processing error: {str(e)}. " if language == 'en' else f"خطای پردازش پرس‌وجو: {str(e)}. "
                print(f"Query processing error: {str(e)}")
        audio_path = None
        visual_output = description
        if user_type == 'blind':
            audio_path = f"audio/output_{int(time.time())}.mp3"
            os.makedirs('audio', exist_ok=True)
            try:
                tts = gTTS(text=description + (" Processing complete." if language == 'en' else " پردازش پایان یافت."), lang=language, slow=False, tld='co.uk')
                tts.save(audio_path)
            except Exception as e:
                print(f"gTTS error: {str(e)}")
                audio_path = None
        elif user_type == 'deaf':
            visual_output = f"<div class='bg-blue-100 p-6 rounded-xl shadow-lg text-lg leading-relaxed'>{description}</div>"
        return description, audio_path, visual_output, objects, False, False, "", None, saved_file_path, base64_file
    except Exception as e:
        print(f"Error in analyze_input: {str(e)}")
        error_msg = f"Error: {str(e)}" if language == 'en' else f"خطا: {str(e)}"
        return error_msg, None, error_msg, [], False, False, "", None, None, None

@app.route('/')
def index():
    return render_template_string(html_content)

@app.route('/generate_audio', methods=['POST'])
def generate_audio():
    try:
        data = request.get_json()
        if not data or 'message' not in data:
            return jsonify({'error': 'Invalid or missing JSON data'}), 400
        message = data.get('message', '')
        language = data.get('language', 'en')
        audio_path = generate_system_audio(message, language)
        if audio_path:
            return jsonify({'audio_url': f'/get_audio/{os.path.basename(audio_path)}'})
        else:
            return jsonify({'error': 'Failed to generate audio'}), 500
    except Exception as e:
        print(f"Generate audio error: {str(e)}")
        return jsonify({'error': f"Generate audio error: {str(e)}"}), 500

@app.route('/process_input', methods=['POST'])
def process_input():
    start_time = time.time()
    language = request.form.get('language', 'en')
    user_type = request.form.get('user_type', 'blind')
    query = request.form.get('query', None)
    user_lat = request.form.get('user_lat')
    user_lon = request.form.get('user_lon')
    try:
        media = None
        audio_path = None
        if 'audio' in request.files:
            audio_file = request.files['audio']
            audio_path = f"temp_audio_{int(time.time())}.webm"
            audio_file.save(audio_path)
            query = transcribe_audio(audio_path, language)
            if os.path.exists(audio_path):
                os.remove(audio_path)
        if 'media' in request.files:
            media = request.files['media']
        elif request.form.get('image_data'):
            data_url = request.form.get('image_data')
            header, encoded = data_url.split(',', 1)
            data = b64decode(encoded)
            media = BytesIO(data)
        description, audio_output, visual_output, objects, take_photo_triggered, warning, warning_message, warning_audio, saved_file_path, base64_file = analyze_input(
            media, audio_path, language, query, user_type, user_lat, user_lon
        )
        processing_time = time.time() - start_time
        response = {
            'description': description,
            'audio_url': f'/get_audio/{os.path.basename(audio_output)}' if audio_output else None,
            'visual_output': visual_output,
            'objects': objects,
            'processing_time': f"{processing_time:.2f} seconds",
            'take_photo_triggered': take_photo_triggered,
            'warning': warning,
            'warning_message': warning_message,
            'warning_audio_url': f'/get_audio/{os.path.basename(warning_audio)}' if warning_audio else None,
            'saved_file_path': saved_file_path,
            'saved_file_url': f'/get_file/{os.path.basename(saved_file_path)}' if saved_file_path else None,
            'file_base64': base64_file
        }
        return jsonify(response)
    except Exception as e:
        print(f"Process input error: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/get_audio/<filename>')
def get_audio(filename):
    try:
        return send_file(os.path.join('audio', filename), mimetype='audio/mp3')
    except Exception as e:
        print(f"Audio file error: {str(e)}")
        return jsonify({'error': f"Audio file error: {str(e)}"}), 500

@app.route('/get_file/<filename>')
def get_file(filename):
    try:
        if filename.endswith('.mp4'):
            return send_file(os.path.join('saved_files', filename), mimetype='video/mp4')
        else:
            return send_file(os.path.join('saved_files', filename), mimetype='image/jpeg')
    except Exception as e:
        print(f"File error: {str(e)}")
        return jsonify({'error': f"File error: {str(e)}"}), 500

ngrok.set_auth_token(os.getenv('NGROK_TOKEN', '2s9UYBDAhKCGblwUTV8PPf5AhJg_75PDRoniQ2hhzvtmoy8SN'))
def keep_alive():
    while True:
        time.sleep(3600)
threading.Thread(target=keep_alive, daemon=True).start()

if __name__ == '__main__':
    http_tunnel = ngrok.connect(5000, bind_tls=True)
    public_url = http_tunnel.public_url
    print(f"Flask app running at: {public_url}")
    app.run(port=5000)

import os
os.kill(os.getpid(), 9)
